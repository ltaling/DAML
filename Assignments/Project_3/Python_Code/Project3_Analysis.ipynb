{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf32623b",
   "metadata": {},
   "source": [
    "# Project 3 Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d15696a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing all the required imports\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.keras.utils.vis_utils import plot_model\n",
    "from tensorflow.python.keras.models import Model\n",
    "from tensorflow.python.keras.layers import Input, Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Input\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from tensorflow.keras import regularizers\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a68aa1",
   "metadata": {},
   "source": [
    "# Part 1 -- Particle Detection With a Homogeneous and Sampling Calorimeter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16e881d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the data files -- All energies are in MeV\n",
    "\n",
    "electrons_100mev = pd.read_csv( \"Electrons_100mev.csv\", comment=\"#\",\\\n",
    "names=[ \"True Energy\", \"Homogeneous\", \"Layer 1\",\"Layer 2\", \"Layer 3\",\"Layer 4\",\"Layer 5\" ] )\n",
    "\n",
    "protons_100mev = pd.read_csv( \"Protons_100mev.csv\", comment=\"#\",\\\n",
    "names=[ \"True Energy\", \"Homogeneous\", \"Layer 1\",\"Layer 2\", \"Layer 3\",\"Layer 4\",\"Layer 5\" ] )\n",
    "\n",
    "neutrons_100mev = pd.read_csv( \"Neutrons_100mev.csv\", comment=\"#\",\\\n",
    "names=[ \"True Energy\", \"Homogeneous\", \"Layer 1\",\"Layer 2\", \"Layer 3\",\"Layer 4\",\"Layer 5\" ] )\n",
    "\n",
    "photons_100mev = pd.read_csv( \"Photons_100mev.csv\", comment=\"#\",\\\n",
    "names=[ \"True Energy\", \"Homogeneous\", \"Layer 1\",\"Layer 2\", \"Layer 3\",\"Layer 4\",\"Layer 5\" ] )\n",
    "\n",
    "electrons_10gev = pd.read_csv( \"Electrons_10gev.csv\", comment=\"#\",\\\n",
    "names=[ \"True Energy\", \"Homogeneous\", \"Layer 1\",\"Layer 2\", \"Layer 3\",\"Layer 4\",\"Layer 5\" ] )\n",
    "\n",
    "protons_10gev = pd.read_csv( \"Protons_10gev.csv\", comment=\"#\",\\\n",
    "names=[ \"True Energy\", \"Homogeneous\", \"Layer 1\",\"Layer 2\", \"Layer 3\",\"Layer 4\",\"Layer 5\" ] )\n",
    "\n",
    "neutrons_10gev = pd.read_csv( \"Neutrons_10gev.csv\", comment=\"#\",\\\n",
    "names=[ \"True Energy\", \"Homogeneous\", \"Layer 1\",\"Layer 2\", \"Layer 3\",\"Layer 4\",\"Layer 5\" ] )\n",
    "\n",
    "photons_10gev = pd.read_csv( \"Photons_10gev.csv\", comment=\"#\",\\\n",
    "names=[ \"True Energy\", \"Homogeneous\", \"Layer 1\",\"Layer 2\", \"Layer 3\",\"Layer 4\",\"Layer 5\" ] )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "597992dd",
   "metadata": {},
   "source": [
    "## Particle Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449ea121",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, we append the particle id onto each column\n",
    "# 0 = electrons\n",
    "# 1 = protons\n",
    "# 2 = neutrons\n",
    "# 3 = photons\n",
    "\n",
    "# Define the particle id numbers\n",
    "e = np.zeros(len(electrons_100mev),dtype=int)\n",
    "p = np.ones(len(protons_100mev),dtype=int)\n",
    "n = 2*np.ones(len(neutrons_100mev),dtype=int)\n",
    "g = 3*np.ones(len(photons_100mev),dtype=int)\n",
    "\n",
    "# Append it to the corresponding dataframe\n",
    "electrons_100mev[\"Particle ID\"] = e.tolist()\n",
    "# Re-order columns to make it look better\n",
    "electrons_100mev = electrons_100mev[[\"Particle ID\",\"True Energy\",\\\n",
    "                   \"Homogeneous\", \"Layer 1\",\"Layer 2\", \"Layer 3\",\"Layer 4\",\"Layer 5\"]]\n",
    "\n",
    "protons_100mev[\"Particle ID\"] = p.tolist()\n",
    "protons_100mev = protons_100mev[[\"Particle ID\",\"True Energy\",\\\n",
    "                   \"Homogeneous\", \"Layer 1\",\"Layer 2\", \"Layer 3\",\"Layer 4\",\"Layer 5\"]]\n",
    "\n",
    "neutrons_100mev[\"Particle ID\"] = n.tolist()\n",
    "neutrons_100mev = neutrons_100mev[[\"Particle ID\",\"True Energy\",\\\n",
    "                   \"Homogeneous\", \"Layer 1\",\"Layer 2\", \"Layer 3\",\"Layer 4\",\"Layer 5\"]]\n",
    "\n",
    "photons_100mev[\"Particle ID\"] = g.tolist()\n",
    "photons_100mev = photons_100mev[[\"Particle ID\",\"True Energy\",\\\n",
    "                   \"Homogeneous\", \"Layer 1\",\"Layer 2\", \"Layer 3\",\"Layer 4\",\"Layer 5\"]]\n",
    "\n",
    "electrons_10gev[\"Particle ID\"] = e.tolist()\n",
    "electrons_10gev = electrons_10gev[[\"Particle ID\",\"True Energy\",\\\n",
    "                   \"Homogeneous\", \"Layer 1\",\"Layer 2\", \"Layer 3\",\"Layer 4\",\"Layer 5\"]]\n",
    "\n",
    "protons_10gev[\"Particle ID\"] = p.tolist()\n",
    "protons_10gev = protons_10gev[[\"Particle ID\",\"True Energy\",\\\n",
    "                   \"Homogeneous\", \"Layer 1\",\"Layer 2\", \"Layer 3\",\"Layer 4\",\"Layer 5\"]]\n",
    "\n",
    "neutrons_10gev[\"Particle ID\"] = n.tolist()\n",
    "neutrons_10gev = neutrons_10gev[[\"Particle ID\",\"True Energy\",\\\n",
    "                   \"Homogeneous\", \"Layer 1\",\"Layer 2\", \"Layer 3\",\"Layer 4\",\"Layer 5\"]]\n",
    "\n",
    "photons_10gev[\"Particle ID\"] = g.tolist()\n",
    "photons_10gev = photons_10gev[[\"Particle ID\",\"True Energy\",\\\n",
    "                   \"Homogeneous\", \"Layer 1\",\"Layer 2\", \"Layer 3\",\"Layer 4\",\"Layer 5\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c974174",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append all the dataframes together by energy and shuffle to create our dataset\n",
    "\n",
    "# Appending\n",
    "particles_100mev = pd.concat([electrons_100mev,protons_100mev,neutrons_100mev,photons_100mev],ignore_index=True)\n",
    "particles_10gev = pd.concat([electrons_10gev,protons_10gev,neutrons_10gev,photons_10gev],ignore_index=True)\n",
    "\n",
    "#Shuffling\n",
    "particles_100mev = particles_100mev.sample(frac=1).reset_index(drop=True)\n",
    "particles_10gev = particles_10gev.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1dce5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check to see if our data frame looks good\n",
    "particles_10gev\n",
    "# It looks great!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a81e871",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create our testing and training datasets\n",
    "\n",
    "#100 mev particles\n",
    "y_100 = particles_100mev[[\"Particle ID\"]] # Target\n",
    "x_100 = particles_100mev[[\"Homogeneous\",\"Layer 1\",\"Layer 2\", \"Layer 3\",\"Layer 4\",\"Layer 5\"]] # Parameters\n",
    "\n",
    "#10 gev particles\n",
    "y_10 = particles_10gev[[\"Particle ID\"]]\n",
    "x_10 = particles_10gev[[\"Homogeneous\",\"Layer 1\",\"Layer 2\", \"Layer 3\",\"Layer 4\",\"Layer 5\"]]\n",
    "\n",
    "x100m_train, x100m_test, y100m_train, y100m_test = train_test_split(x_100,y_100, test_size=0.3,\n",
    "                                                        random_state=1) # 70% training and 30% test\n",
    "\n",
    "x10g_train, x10g_test, y10g_train, y10g_test = train_test_split(x_10,y_10, test_size=0.3,\n",
    "                                                        random_state=1) # 70% training and 30% test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7469fb07",
   "metadata": {},
   "outputs": [],
   "source": [
    "y100m_train # The target dataset looks good!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d000d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the neural network\n",
    "\n",
    "def model(shape):\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    # Add input with shape equal to 6 (number of energy counters)\n",
    "    model.add(Input(shape=(shape,)))\n",
    "    # Dense layer with 100 inputs and relu activation function\n",
    "    model.add(Dense(100,kernel_initializer = 'normal',activation ='relu'))\n",
    "    # Dropout layer to reduce overtraining\n",
    "    model.add(Dropout(0.1)) \n",
    "    # Dense layer with 100 inputs and relu activation function\n",
    "    model.add(Dense(100,kernel_initializer = 'normal',activation ='relu'))\n",
    "    # Dropout layer to reduce overtraining\n",
    "    model.add(Dropout(0.1))\n",
    "    # output layer with 4 inputs and softmax activation function for categorisation\n",
    "    model.add(Dense(4,kernel_initializer ='normal',activation='softmax'))\n",
    "    # Compile model with SCC loss function as this is a classification NN\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='Adam',metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model_100m = model(np.shape(x100m_train)[1]) # Activate 100 MeV model\n",
    "model_10g = model(np.shape(x10g_train)[1]) # Activate 10 GeV model\n",
    "\n",
    "model_100m.summary() # The model looks acceptable and has over 11,000 trainable parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05fb1e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the 100 MeV model with 512 batch size, 20% validation split and 100 epochs\n",
    "history_100m = model_100m.fit(x=x100m_train,y=y100m_train,batch_size=512,\\\n",
    "                                              epochs=120,validation_split=0.2,shuffle=True)\n",
    "\n",
    "# Fit the 10 GeV model with 128 batch size, 20% validation split and 100 epochs\n",
    "history_10g = model_10g.fit(x=x10g_train,y=y10g_train,batch_size=128,\\\n",
    "                                              epochs=100,validation_split=0.2,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d81ac66",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting the loss curves\n",
    "\n",
    "fig,ax = plt.subplots(2,2,figsize=(12,9))\n",
    "ax[0,0].plot(history_100m.history['loss'],label='Loss')\n",
    "ax[0,0].plot(history_100m.history['val_loss'],label='Validation Loss')\n",
    "ax[0,0].set_title(\"Loss Change over Time (100 MeV)\")\n",
    "ax[0,0].set_xlabel(\"Epoch\")\n",
    "ax[0,0].set_ylabel(\"Loss\")\n",
    "ax[0,0].legend()\n",
    "\n",
    "ax[1,0].plot(history_100m.history['accuracy'],label='Accuracy')\n",
    "ax[1,0].plot(history_100m.history['val_accuracy'],label='Validation Accuracy')\n",
    "ax[1,0].set_title(\"Accuracy Change over Time (100 MeV)\")\n",
    "ax[1,0].set_xlabel(\"Epoch\")\n",
    "ax[1,0].set_ylabel(\"Accuracy\")\n",
    "ax[1,0].legend()\n",
    "\n",
    "ax[0,1].plot(history_10g.history['loss'],label='Loss')\n",
    "ax[0,1].plot(history_10g.history['val_loss'],label='Validation Loss')\n",
    "ax[0,1].set_title(\"Loss Change over Time (10 GeV)\")\n",
    "ax[0,1].set_xlabel(\"Epoch\")\n",
    "ax[0,1].set_ylabel(\"Loss\")\n",
    "ax[0,1].legend()\n",
    "\n",
    "ax[1,1].plot(history_10g.history['accuracy'],label='Accuracy')\n",
    "ax[1,1].plot(history_10g.history['val_accuracy'],label='Validation Accuracy')\n",
    "ax[1,1].set_title(\"Accuracy Change over Time (10 GeV)\")\n",
    "ax[1,1].set_xlabel(\"Epoch\")\n",
    "ax[1,1].set_ylabel(\"Accuracy\")\n",
    "ax[1,1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87dc8b61",
   "metadata": {},
   "source": [
    "These loss curves are very satisfactory as they decrease to values of around 0.6-0.7 and show no signs of overtraining (the validation loss is similar to the loss). The accuracy also tends to around 70% for both energies too. The higher energy one, however, is slightly more stable than the lower energy one as it fluctuates less. It used to fluctuate more in earlier runs when the same batch size was used (128) but with such a large dataset that differs in detected energy a lot (c.f. energy resolution calculations below), a small batch size would cause large fluctuations as the various data would have huge influences rather than being cancelled out via the safety of large numbers. The large batch size seems to work except for the 2 outrageous peaks* in the validation accuracy although, considering these only appear in the validation and not in the training set, it is probably due to the fact that only 20% of the data is used in the validation set so it is likely that due to the random nature of it, a disproportionate amount of the the difficult particles (e.g. photons) have been included. \n",
    "\n",
    "Furthermore, there is no obvious difference between the distinction of particles for the high and low energy particles which is mildly surprising. It was expected that lower energy particles would be easier to distinguish as they are always absorbed in the detector whereas the higher energy ones often pass straight through. Presumably these effects cancelled each other out, as the neural network used this phenomenon as a marker to differentiate the highly penetrating particles from those that are easily absorbed. \n",
    "\n",
    "\\*after running the code again, there may be no peaks or more of them -- it changes every run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1396dc73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See how well the model predicts the testing data set\n",
    "prediction_100m = np.argmax(model_100m.predict(x100m_test),axis=1)\n",
    "prediction_10g = np.argmax(model_10g.predict(x10g_test),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd79f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "fig,ax = plt.subplots(1,2,figsize=(12,5))\n",
    "#Define the confusion matrix and normalise it so that each row/column sums to 1\n",
    "cmat100m = confusion_matrix(y100m_test,prediction_100m,normalize='true')\n",
    "cmat10g = confusion_matrix(y10g_test,prediction_10g,normalize='true')\n",
    "#dispay the matrix and redefine the display labels to show the particle type\n",
    "cmatplot100m = ConfusionMatrixDisplay(confusion_matrix=cmat100m,display_labels=['electron','proton','neutron','photon'])\n",
    "cmatplot10g = ConfusionMatrixDisplay(confusion_matrix=cmat10g,display_labels=['electron','proton','neutron','photon'])\n",
    "\n",
    "cmatplot100m.plot(ax=ax[0])\n",
    "ax[0].set_title(\"100 MeV\")\n",
    "cmatplot10g.plot(ax=ax[1])\n",
    "ax[1].set_title(\"10 GeV\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd7a4735",
   "metadata": {},
   "source": [
    "From these confusion matrices, it is clear to see that each energy had its strengths and weaknesses. Starting with 100 MeV, it does a fantastic job at differentiating between the hadrons, neutrons and protons, but not very good with the elementary particles, electrons and photons. 100 MeV is almost 1/10 the mass of the hadrons whilst 200 times the mass of the electron and deep into the gamma region of the EM spectrum. As a result, most of the protons neatly deposit their energy in both the homogeneous & sampling calorimeters whilst the neutron deposits in only the sampling calorimeter and the electrons & photons don't deposit energy in the sampling calorimeter at all. The model detects this and is therefore able to make good distinction between the protons and neutrons. The electrons and photons, however, are harder to differentiate because at low energies, the photon penetration is similar to that of the electron and they both shower in the homogeneous calorimeter but just do not have enough energy to penetrate into the sampling calorimeter. Thus, their energy deposits are incredibly similar and the neural network has found it easier to assume most particles are electrons hence the 28% efficiency of detecting photons and 60% chance it confuses it for an electron. I suspect the model predicts photons when they penetrate the first layer of the sampling calorimeter, but when energy is deposited in the homogeneous one and not the sampling one, then it assumes it's an electron. Therefore, it detects more electrons.\n",
    "\n",
    "At 10 GeV, it is worse at differentiating between neutrons and protons but can still differentiate the hadrons from the elementary particles. Likely 10 GeV has increased the penetrating power so much that many protons simply pass through the homogeneous calorimeter without depositing much energy, thus making them indistinguishable from neutrons. Moreover, many particles simply escape the detectors altogether due to their high energies so it could be that a lot of the confusion comes from both types of particle depositing little energy. Conversely, the electron-photon differentiation is superior at high energies, likely due to the fact that the photon penetrating power is so high that it manages to pass through the lead plates of the sampling calorimeter and deposit there. The electron still deposits too much energy in the homogeneous one to penetrate hence the neural network is better.\n",
    "\n",
    "To improve particle identification, an additional layer that induces specific particle reactions could be used. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e687ad7c",
   "metadata": {},
   "source": [
    "## Energy Resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0939caf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the energy resolution for each particle separately and then all of them together\n",
    "\n",
    "# Getting the data required \n",
    "E_true_100 = electrons_100mev['True Energy'] # Same for every particle\n",
    "E_true_10g = electrons_10gev['True Energy']\n",
    "\n",
    "# Total detected energy\n",
    "E_electron_det_100 = electrons_100mev[[\"Homogeneous\",\"Layer 1\",\"Layer 2\", \"Layer 3\",\"Layer 4\",\"Layer 5\"]].sum(axis=1)\n",
    "E_proton_det_100 = protons_100mev[[\"Homogeneous\",\"Layer 1\",\"Layer 2\", \"Layer 3\",\"Layer 4\",\"Layer 5\"]].sum(axis=1)\n",
    "E_neutron_det_100 = neutrons_100mev[[\"Homogeneous\",\"Layer 1\",\"Layer 2\", \"Layer 3\",\"Layer 4\",\"Layer 5\"]].sum(axis=1)\n",
    "E_photon_det_100 = photons_100mev[[\"Homogeneous\",\"Layer 1\",\"Layer 2\", \"Layer 3\",\"Layer 4\",\"Layer 5\"]].sum(axis=1)\n",
    "E_total_det_100 = pd.concat([E_electron_det_100,E_proton_det_100,E_neutron_det_100,E_photon_det_100],\\\n",
    "                           ignore_index=True)\n",
    "\n",
    "E_electron_det_10g = electrons_10gev[[\"Homogeneous\",\"Layer 1\",\"Layer 2\", \"Layer 3\",\"Layer 4\",\"Layer 5\"]].sum(axis=1)\n",
    "E_proton_det_10g = protons_10gev[[\"Homogeneous\",\"Layer 1\",\"Layer 2\", \"Layer 3\",\"Layer 4\",\"Layer 5\"]].sum(axis=1)\n",
    "E_neutron_det_10g = neutrons_10gev[[\"Homogeneous\",\"Layer 1\",\"Layer 2\", \"Layer 3\",\"Layer 4\",\"Layer 5\"]].sum(axis=1)\n",
    "E_photon_det_10g = photons_10gev[[\"Homogeneous\",\"Layer 1\",\"Layer 2\", \"Layer 3\",\"Layer 4\",\"Layer 5\"]].sum(axis=1)\n",
    "E_total_det_10g = pd.concat([E_electron_det_10g,E_proton_det_10g,E_neutron_det_10g,E_photon_det_10g],\\\n",
    "                           ignore_index=True)\n",
    "\n",
    "# Remove zero values to avoid divide by 0 errors\n",
    "E_electron_det_100 = E_electron_det_100.drop(np.where(E_electron_det_100==0)[0])\n",
    "E_proton_det_100 = E_proton_det_100.drop(np.where(E_proton_det_100==0)[0])\n",
    "E_neutron_det_100 = E_neutron_det_100.drop(np.where(E_neutron_det_100==0)[0])\n",
    "E_photon_det_100 = E_photon_det_100.drop(np.where(E_photon_det_100==0)[0])\n",
    "E_total_det_100 = E_total_det_100.drop(np.where(E_total_det_100==0)[0])\n",
    "\n",
    "E_electron_det_10g = E_electron_det_10g.drop(np.where(E_electron_det_10g==0)[0])\n",
    "E_proton_det_10g = E_proton_det_10g.drop(np.where(E_proton_det_10g==0)[0])\n",
    "E_neutron_det_10g = E_neutron_det_10g.drop(np.where(E_neutron_det_10g==0)[0])\n",
    "E_photon_det_10g = E_photon_det_10g.drop(np.where(E_photon_det_10g==0)[0])\n",
    "E_total_det_10g = E_total_det_10g.drop(np.where(E_total_det_10g==0)[0])\n",
    "\n",
    "# Calibrate the energy\n",
    "E_electron_cal_100 = np.mean(E_true_100[:len(E_electron_det_100)]/np.asarray(E_electron_det_100))*E_electron_det_100 \n",
    "E_proton_cal_100 = np.mean(E_true_100[:len(E_proton_det_100)]/np.asarray(E_proton_det_100))*E_proton_det_100\n",
    "E_neutron_cal_100 = np.mean(E_true_100[:len(E_neutron_det_100)]/np.asarray(E_neutron_det_100))*E_neutron_det_100\n",
    "E_photon_cal_100 = np.mean(E_true_100[:len(E_photon_det_100)]/np.asarray(E_photon_det_100))*E_photon_det_100\n",
    "E_total_cal_100 = np.mean(np.asarray(4*list(E_true_100))[:len(E_total_det_100)]/np.asarray(E_total_det_100))*E_total_det_100\n",
    "\n",
    "E_electron_cal_10g = np.mean(E_true_10g[:len(E_electron_det_10g)]/np.asarray(E_electron_det_10g))*E_electron_det_10g \n",
    "E_proton_cal_10g = np.mean(E_true_10g[:len(E_proton_det_10g)]/np.asarray(E_proton_det_10g))*E_proton_det_10g\n",
    "E_neutron_cal_10g = np.mean(E_true_10g[:len(E_neutron_det_10g)]/np.asarray(E_neutron_det_10g))*E_neutron_det_10g\n",
    "E_photon_cal_10g = np.mean(E_true_10g[:len(E_photon_det_10g)]/np.asarray(E_photon_det_10g))*E_photon_det_10g\n",
    "E_total_cal_10g = np.mean(np.asarray(4*list(E_true_10g))[:len(E_total_det_10g)]/np.asarray(E_total_det_10g))*E_total_det_10g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bacdc7ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the weighted differences of the calibrated and true energy\n",
    "electron_diff_100 = (np.asarray(E_electron_cal_100)-E_true_100[:len(E_electron_det_100)])/E_true_100[:len(E_electron_det_100)]\n",
    "proton_diff_100 = (np.asarray(E_proton_cal_100)-E_true_100[:len(E_proton_det_100)])/E_true_100[:len(E_proton_det_100)]\n",
    "neutron_diff_100 = (np.asarray(E_neutron_cal_100)-E_true_100[:len(E_neutron_det_100)])/E_true_100[:len(E_neutron_det_100)]\n",
    "photon_diff_100 = (np.asarray(E_photon_cal_100)-E_true_100[:len(E_photon_det_100)])/E_true_100[:len(E_photon_det_100)]\n",
    "total_diff_100 = (np.asarray(E_total_cal_100)-np.asarray(4*list(E_true_100))[:len(E_total_det_100)])\\\n",
    "                    /np.asarray(4*list(E_true_100))[:len(E_total_det_100)]\n",
    "\n",
    "electron_diff_10g = (np.asarray(E_electron_cal_10g)-E_true_10g[:len(E_electron_det_10g)])/E_true_10g[:len(E_electron_det_10g)]\n",
    "proton_diff_10g = (np.asarray(E_proton_cal_10g)-E_true_10g[:len(E_proton_det_10g)])/E_true_10g[:len(E_proton_det_10g)]\n",
    "neutron_diff_10g = (np.asarray(E_neutron_cal_10g)-E_true_10g[:len(E_neutron_det_10g)])/E_true_10g[:len(E_neutron_det_10g)]\n",
    "photon_diff_10g = (np.asarray(E_photon_cal_10g)-E_true_10g[:len(E_photon_det_10g)])/E_true_10g[:len(E_photon_det_10g)]\n",
    "total_diff_10g = (np.asarray(E_total_cal_10g)-np.asarray(4*list(E_true_10g))[:len(E_total_det_10g)])\\\n",
    "                    /np.asarray(4*list(E_true_10g))[:len(E_total_det_10g)]\n",
    "\n",
    "# Calculating the energy resolution\n",
    "electron_res_100 = np.std(electron_diff_100)\n",
    "proton_res_100 = np.std(proton_diff_100)\n",
    "neutron_res_100 = np.std(neutron_diff_100)\n",
    "photon_res_100 = np.std(photon_diff_100)\n",
    "total_res_100 = np.std(total_diff_100)\n",
    "\n",
    "electron_res_10g = np.std(electron_diff_10g)\n",
    "proton_res_10g = np.std(proton_diff_10g)\n",
    "neutron_res_10g = np.std(neutron_diff_10g)\n",
    "photon_res_10g = np.std(photon_diff_10g)\n",
    "total_res_10g = np.std(total_diff_10g)\n",
    "\n",
    "#Plotting the Histograms\n",
    "fig,ax = plt.subplots(2,2,figsize=(12,9))\n",
    "\n",
    "ax[0,0].hist(electron_diff_100,label='100 MeV Resolution = {:.4f}'.format(electron_res_100),bins=50)\n",
    "ax[0,0].hist(electron_diff_10g,label='10 GeV Resolution = {:.4f}'.format(electron_res_10g),alpha=0.9,bins=50)\n",
    "ax[0,0].set_title(\"Calibration Energy histogram of Electrons\")\n",
    "ax[0,0].set_xlabel(\"($E_{cal}-E_{true})/E_{true}$\")\n",
    "ax[0,0].set_ylabel(\"Frequency\")\n",
    "ax[0,0].legend()\n",
    "\n",
    "ax[1,0].hist(proton_diff_100,label='100 MeV Resolution = {:.4f}'.format(proton_res_100),bins=10)\n",
    "ax[1,0].hist(proton_diff_10g,label='10 GeV Resolution = {:.4f}'.format(proton_res_10g),alpha=0.9,bins=50)\n",
    "ax[1,0].set_title(\"Calibration Energy histogram of Protons\")\n",
    "ax[1,0].set_xlabel(\"($E_{cal}-E_{true})/E_{true}$\")\n",
    "ax[1,0].set_ylabel(\"Frequency\")\n",
    "ax[1,0].legend()\n",
    "\n",
    "ax[0,1].hist(neutron_diff_100,label='100 MeV Resolution = {:.4f}'.format(neutron_res_100),bins=50)\n",
    "ax[0,1].hist(neutron_diff_10g,label='10 GeV Resolution = {:.4f}'.format(neutron_res_10g),alpha=0.8,bins=50)\n",
    "ax[0,1].set_title(\"Calibration Energy histogram of Neutrons\")\n",
    "ax[0,1].set_xlabel(\"($E_{cal}-E_{true})/E_{true}$\")\n",
    "ax[0,1].set_ylabel(\"Frequency\")\n",
    "ax[0,1].set_xscale('log') # Logarithmic scale to see the resolution better\n",
    "ax[0,1].legend()\n",
    "\n",
    "ax[1,1].hist(photon_diff_100,label='100 MeV Resolution = {:.4f}'.format(photon_res_100),bins=50)\n",
    "ax[1,1].hist(photon_diff_10g,label='10 GeV Resolution = {:.4f}'.format(photon_res_10g),alpha=0.8,bins=50)\n",
    "ax[1,1].set_title(\"Calibration Energy histogram of Photons\")\n",
    "ax[1,1].set_xlabel(\"($E_{cal}-E_{true})/E_{true}$\")\n",
    "ax[1,1].set_ylabel(\"Frequency\")\n",
    "ax[1,1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.hist(total_diff_100,label='100 MeV Resolution = {:.4f}'.format(total_res_100),bins=50)\n",
    "plt.hist(total_diff_10g,label='10 GeV Resolution = {:.4f}'.format(total_res_10g),alpha=0.8,bins=50)\n",
    "plt.title(\"Calibration Energy Histogram of all Particles\")\n",
    "plt.xlabel(\"($E_{cal}-E_{true})/E_{true}$\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.xscale('log') \n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "181335b5",
   "metadata": {},
   "source": [
    "As can be seen in the graphs above, the energy resolution is terrible -- especially for uncharged particles which see their resolutions 2 orders of magnitude greater than the charged ones. The reason for this is because uncharged particles go straight through the material without interacting much and therefore do not deposit as much energy. This reduces the resolution dramatically. This becomes incredibly apparent at high energies as the orange graphs are shifted significantly to the right, so much for the photons & neutrons that the scale must be logarithmic to even provide more than a single line for the low energy. In more positive news, however, the 100 MeV protons and electrons have acceptable resolutions which are gaussian centred around 0. On top of this, the 10 GeV photons also have a very good resolution which seems surprising at first but, upon further thinking, it is potentially because the photons shower enough in the lead layer that the pair-produced particles have enough energy to deposit it into the sampling calorimeter thus less energy is missing. On top of this, fewer photons may be scattering away as pair production is the solely dominant effect at this energy range (rather than compton scattering)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4245031",
   "metadata": {},
   "source": [
    "# Extension Part\n",
    "\n",
    "## Using a Graphite target\n",
    "\n",
    "In this section, a graphite target is used to induce scattering/interaction events before entering the calorimeters to aid in the classification of particles. The motivation behind this is that greater range of particles produced in the interactions (or the lack of interactions) will give more clues for the neural network to guess which particle is responsible for the energy deposited. On top of this, energy loss via scattering within the target will also offer clues to the neural network on top of the regular data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d77974e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the data files -- All energies are in MeV\n",
    "\n",
    "# Get rid of column 0 which contains the true energy\n",
    "electrons_100mev_Ctarget = pd.read_csv( \"Electrons_100mev_Ctarget.csv\", comment=\"#\",usecols=[1,2,3,4,5,6],\\\n",
    "names=[ \"HomogeneousCT\", \"Layer 1CT\",\"Layer 2CT\", \"Layer 3CT\",\"Layer 4CT\",\"Layer 5CT\" ] )\n",
    "\n",
    "protons_100mev_Ctarget = pd.read_csv( \"Protons_100mev_Ctarget.csv\", comment=\"#\",usecols=[1,2,3,4,5,6],\\\n",
    "names=[ \"HomogeneousCT\", \"Layer 1CT\",\"Layer 2CT\", \"Layer 3CT\",\"Layer 4CT\",\"Layer 5CT\" ] )\n",
    "\n",
    "neutrons_100mev_Ctarget = pd.read_csv( \"Neutrons_100mev_Ctarget.csv\", comment=\"#\",usecols=[1,2,3,4,5,6],\\\n",
    "names=[ \"HomogeneousCT\", \"Layer 1CT\",\"Layer 2CT\", \"Layer 3CT\",\"Layer 4CT\",\"Layer 5CT\" ] )\n",
    "\n",
    "photons_100mev_Ctarget = pd.read_csv( \"Photons_100mev_Ctarget.csv\", comment=\"#\",usecols=[1,2,3,4,5,6],\\\n",
    "names=[ \"HomogeneousCT\", \"Layer 1CT\",\"Layer 2CT\", \"Layer 3CT\",\"Layer 4CT\",\"Layer 5CT\" ] )\n",
    "\n",
    "electrons_10gev_Ctarget = pd.read_csv( \"Electrons_10gev_Ctarget.csv\", comment=\"#\",usecols=[1,2,3,4,5,6],\\\n",
    "names=[ \"HomogeneousCT\", \"Layer 1CT\",\"Layer 2CT\", \"Layer 3CT\",\"Layer 4CT\",\"Layer 5CT\" ] )\n",
    "\n",
    "protons_10gev_Ctarget = pd.read_csv( \"Protons_10gev_Ctarget.csv\", comment=\"#\",usecols=[1,2,3,4,5,6],\\\n",
    "names=[ \"HomogeneousCT\", \"Layer 1CT\",\"Layer 2CT\", \"Layer 3CT\",\"Layer 4CT\",\"Layer 5CT\" ] )\n",
    "\n",
    "neutrons_10gev_Ctarget = pd.read_csv( \"Neutrons_10gev_Ctarget.csv\", comment=\"#\",usecols=[1,2,3,4,5,6],\\\n",
    "names=[ \"HomogeneousCT\", \"Layer 1CT\",\"Layer 2CT\", \"Layer 3CT\",\"Layer 4CT\",\"Layer 5CT\" ] )\n",
    "\n",
    "photons_10gev_Ctarget = pd.read_csv( \"Photons_10gev_Ctarget.csv\", comment=\"#\",usecols=[1,2,3,4,5,6],\\\n",
    "names=[ \"HomogeneousCT\", \"Layer 1CT\",\"Layer 2CT\", \"Layer 3CT\",\"Layer 4CT\",\"Layer 5CT\" ] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3414d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append the data alongside the non-target data.\n",
    "\n",
    "graphite_electrons_100mev = pd.concat([electrons_100mev,electrons_100mev_Ctarget],axis=1)\n",
    "graphite_protons_100mev = pd.concat([protons_100mev,protons_100mev_Ctarget],axis=1)\n",
    "graphite_neutrons_100mev = pd.concat([neutrons_100mev,neutrons_100mev_Ctarget],axis=1)\n",
    "graphite_photons_100mev = pd.concat([photons_100mev,photons_100mev_Ctarget],axis=1)\n",
    "graphite_electrons_10gev = pd.concat([electrons_10gev,electrons_10gev_Ctarget],axis=1)\n",
    "graphite_protons_10gev = pd.concat([protons_10gev,protons_10gev_Ctarget],axis=1)\n",
    "graphite_neutrons_10gev = pd.concat([neutrons_10gev,neutrons_10gev_Ctarget],axis=1)\n",
    "graphite_photons_10gev = pd.concat([photons_10gev,photons_10gev_Ctarget],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a127a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append all the dataframes together by energy and shuffle to create our dataset\n",
    "\n",
    "# Appending\n",
    "graphite_particles_100mev = pd.concat([graphite_electrons_100mev,graphite_protons_100mev,graphite_neutrons_100mev,\\\n",
    "                                  graphite_photons_100mev],ignore_index=True)\n",
    "graphite_particles_10gev = pd.concat([graphite_electrons_10gev,graphite_protons_10gev,graphite_neutrons_10gev,\\\n",
    "                                 graphite_photons_10gev],ignore_index=True)\n",
    "\n",
    "# Shuffling\n",
    "graphite_particles_100mev = graphite_particles_100mev.sample(frac=1).reset_index(drop=True)\n",
    "graphite_particles_10gev = graphite_particles_10gev.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c42b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check to see if our data frame looks good\n",
    "graphite_particles_10gev\n",
    "# It looks great!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded9a5ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create our testing and training datasets\n",
    "\n",
    "Ct_y_100 = graphite_particles_100mev[[\"Particle ID\"]] # Target\n",
    "Ct_x_100 = graphite_particles_100mev[[\"Homogeneous\",\"Layer 1\",\"Layer 2\", \"Layer 3\",\"Layer 4\",\"Layer 5\",\\\n",
    "                           \"HomogeneousCT\", \"Layer 1CT\",\"Layer 2CT\", \"Layer 3CT\",\"Layer 4CT\",\"Layer 5CT\"]] # Parameters\n",
    "\n",
    "Ct_y_10 = graphite_particles_10gev[[\"Particle ID\"]]\n",
    "Ct_x_10 = graphite_particles_10gev[[\"Homogeneous\",\"Layer 1\",\"Layer 2\", \"Layer 3\",\"Layer 4\",\"Layer 5\",\\\n",
    "                         \"HomogeneousCT\", \"Layer 1CT\",\"Layer 2CT\", \"Layer 3CT\",\"Layer 4CT\",\"Layer 5CT\"]]\n",
    "\n",
    "Ct_x100m_train, Ct_x100m_test, Ct_y100m_train, Ct_y100m_test = train_test_split(Ct_x_100,Ct_y_100, test_size=0.3,\n",
    "                                                        random_state=1) # 70% training and 30% test\n",
    "\n",
    "Ct_x10g_train, Ct_x10g_test, Ct_y10g_train, Ct_y10g_test = train_test_split(Ct_x_10,Ct_y_10, test_size=0.3,\n",
    "                                                        random_state=1) # 70% training and 30% test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6cbdfad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Use the same model structure but now with more data!\n",
    "\n",
    "model_Ct_100m = model(np.shape(Ct_x100m_train)[1]) # 100 mev\n",
    "model_Ct_10g = model(np.shape(Ct_x10g_train)[1]) # 10 gev\n",
    "\n",
    "model_Ct_100m.summary()\n",
    "# This looks the same so all is well "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b507dfe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For 100 mev, use 100 epochs with 20% validation split and batch size 128 (this loss is less jagged)\n",
    "history_Ct_100m = model_Ct_100m.fit(x=Ct_x100m_train,y=Ct_y100m_train,batch_size=128,\\\n",
    "                                              epochs=100,validation_split=0.2,shuffle=True)\n",
    "\n",
    "# For 10 gev, use the same parameters as above\n",
    "history_Ct_10g = model_Ct_10g.fit(x=Ct_x10g_train,y=Ct_y10g_train,batch_size=128,\\\n",
    "                                              epochs=100,validation_split=0.2,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9486096f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting the loss curves\n",
    "\n",
    "fig,ax = plt.subplots(2,2,figsize=(12,9))\n",
    "ax[0,0].plot(history_Ct_100m.history['loss'],label='Loss')\n",
    "ax[0,0].plot(history_Ct_100m.history['val_loss'],label='Validation Loss')\n",
    "ax[0,0].set_title(\"Loss Change over Time (100 MeV)\")\n",
    "ax[0,0].set_xlabel(\"Epoch\")\n",
    "ax[0,0].set_ylabel(\"Loss\")\n",
    "ax[0,0].legend()\n",
    "\n",
    "ax[1,0].plot(history_Ct_100m.history['accuracy'],label='Accuracy')\n",
    "ax[1,0].plot(history_Ct_100m.history['val_accuracy'],label='Validation Accuracy')\n",
    "ax[1,0].set_title(\"Accuracy Change over Time (100 MeV)\")\n",
    "ax[1,0].set_xlabel(\"Epoch\")\n",
    "ax[1,0].set_ylabel(\"Accuracy\")\n",
    "ax[1,0].legend()\n",
    "\n",
    "ax[0,1].plot(history_Ct_10g.history['loss'],label='Loss')\n",
    "ax[0,1].plot(history_Ct_10g.history['val_loss'],label='Validation Loss')\n",
    "ax[0,1].set_title(\"Loss Change over Time (10 GeV)\")\n",
    "ax[0,1].set_xlabel(\"Epoch\")\n",
    "ax[0,1].set_ylabel(\"Loss\")\n",
    "ax[0,1].legend()\n",
    "\n",
    "ax[1,1].plot(history_Ct_10g.history['accuracy'],label='Accuracy')\n",
    "ax[1,1].plot(history_Ct_10g.history['val_accuracy'],label='Validation Accuracy')\n",
    "ax[1,1].set_title(\"Accuracy Change over Time (10 GeV)\")\n",
    "ax[1,1].set_xlabel(\"Epoch\")\n",
    "ax[1,1].set_ylabel(\"Accuracy\")\n",
    "ax[1,1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa7b0eeb",
   "metadata": {},
   "source": [
    "From these loss curves, it is found that the accuracy is better than it was without the target for both the energies. The accuracy is between 86%-88% for 100 MeV (compared to 65%-70% without the target) and for 10 GeV, the accuracy lies between 77%-80%  (compared to 68%-70% without the target). This is a great success and it is stronger for 200 MeV than for 10 GeV. This is to be expected as the high energy particles will often pass straight through the target and so won't show much difference in the calorimeters whereas the 100 MeV particles interact more often thus producing differences. Like those above, the loss curves do not show any overtraining and also aren't decreasing any more meaning the number of epochs is sufficient.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e21e5c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the model on the testing data\n",
    "prediction_Ct_100m = np.argmax(model_Ct_100m.predict(Ct_x100m_test),axis=1)\n",
    "prediction_Ct_10g = np.argmax(model_Ct_10g.predict(Ct_x10g_test),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "746bd565",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "fig,ax = plt.subplots(1,2,figsize=(12,5))\n",
    "#Define the confusion matrix and normalise it so that each row/column sums to 1\n",
    "Ct_cmat100m = confusion_matrix(Ct_y100m_test,prediction_Ct_100m,normalize='true')\n",
    "Ct_cmat10g = confusion_matrix(Ct_y10g_test,prediction_Ct_10g,normalize='true')\n",
    "#dispay the matrix and redefine the display labels to show the particle type\n",
    "Ct_cmatplot100m = ConfusionMatrixDisplay(confusion_matrix=Ct_cmat100m,display_labels=['electron','proton','neutron','photon'])\n",
    "Ct_cmatplot10g = ConfusionMatrixDisplay(confusion_matrix=Ct_cmat10g,display_labels=['electron','proton','neutron','photon'])\n",
    "\n",
    "Ct_cmatplot100m.plot(ax=ax[0])\n",
    "ax[0].set_title(\"100 MeV\")\n",
    "Ct_cmatplot10g.plot(ax=ax[1])\n",
    "ax[1].set_title(\"10 GeV\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85db4e06",
   "metadata": {},
   "source": [
    "From these matrices, it is clear to see that all particles saw similar benefits at both energy ranges. The protons are almost entirely distinguishable at 200 MeV and the neutrons also see a >90% efficiency. It is a modest effect compared to the original data but it is hard to improve on 91% so it is still an achievement. At 10 GeV, the hadrons have a lot higher success rate and still exhibit the same confusion effects meaning the main differentiation methods still come from how they interact in the sampling calorimeter. An interesting thing to note is that, previously, the protons and neutrons were very similar (74% & 72%, respectively) but now they are slightly further apart. This could just be a coincidence but it could also be to do with the fact that protons interact more with the target than the neutrons thus creating more EM particles to deposit energy in the homogeneous calorimeter.\n",
    "\n",
    "The largest difference comes from the elementary particles as the photon has now above 60% accuracy at both energies whilst the electron remains comparable. This means that the neural network is no longer labelling every homogeneous calorimeter deposited energy as an electron but is now differentiating between the two. The difference is likely due to the fact that the electrons interact strongly with the graphite as graphite foil is, in fact, used as an electron diffraction grating in many experiments. Thus, particles that deposit a lot of energy is the homogeneous calorimeter are labelled as photons and particles that deposit little (due to scattering) are labelled as electrons. This doesn't appear to be energy dependent as the distinction quality is highly comparable for the two energies.\n",
    "\n",
    "All in all, the addition of target data is a huge success as it improved particle detection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d1bb1e5",
   "metadata": {},
   "source": [
    "## Energy Resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29646c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the energy resolution for each particle separately and then all of them together\n",
    "\n",
    "# Getting the data required \n",
    "E_true_100 = electrons_100mev['True Energy'] # Same for every particle\n",
    "# Total detected energy\n",
    "E_gelectron_det_100 = graphite_electrons_100mev[[\"HomogeneousCT\",\"Layer 1CT\",\"Layer 2CT\", \"Layer 3CT\",\"Layer 4CT\",\"Layer 5CT\"]].sum(axis=1)\n",
    "E_gproton_det_100 = graphite_protons_100mev[[\"HomogeneousCT\",\"Layer 1CT\",\"Layer 2CT\", \"Layer 3CT\",\"Layer 4CT\",\"Layer 5CT\"]].sum(axis=1)\n",
    "E_gneutron_det_100 = graphite_neutrons_100mev[[\"HomogeneousCT\",\"Layer 1CT\",\"Layer 2CT\", \"Layer 3CT\",\"Layer 4CT\",\"Layer 5CT\"]].sum(axis=1)\n",
    "E_gphoton_det_100 = graphite_photons_100mev[[\"HomogeneousCT\",\"Layer 1CT\",\"Layer 2CT\", \"Layer 3CT\",\"Layer 4CT\",\"Layer 5CT\"]].sum(axis=1)\n",
    "E_gtotal_det_100 = pd.concat([E_gelectron_det_100,E_gproton_det_100,E_gneutron_det_100,E_gphoton_det_100],\\\n",
    "                           ignore_index=True)\n",
    "\n",
    "E_gelectron_det_10g = graphite_electrons_10gev[[\"Homogeneous\",\"Layer 1\",\"Layer 2\", \"Layer 3\",\"Layer 4\",\"Layer 5\"]].sum(axis=1)\n",
    "E_gproton_det_10g = graphite_protons_10gev[[\"Homogeneous\",\"Layer 1\",\"Layer 2\", \"Layer 3\",\"Layer 4\",\"Layer 5\"]].sum(axis=1)\n",
    "E_gneutron_det_10g = graphite_neutrons_10gev[[\"Homogeneous\",\"Layer 1\",\"Layer 2\", \"Layer 3\",\"Layer 4\",\"Layer 5\"]].sum(axis=1)\n",
    "E_gphoton_det_10g = graphite_photons_10gev[[\"Homogeneous\",\"Layer 1\",\"Layer 2\", \"Layer 3\",\"Layer 4\",\"Layer 5\"]].sum(axis=1)\n",
    "E_gtotal_det_10g = pd.concat([E_gelectron_det_10g,E_gproton_det_10g,E_gneutron_det_10g,E_gphoton_det_10g],\\\n",
    "                           ignore_index=True)\n",
    "\n",
    "# Remove zero values to avoid divide by 0 errors\n",
    "E_gelectron_det_100 = E_gelectron_det_100.drop(np.where(E_gelectron_det_100==0)[0])\n",
    "E_gproton_det_100 = E_gproton_det_100.drop(np.where(E_gproton_det_100==0)[0])\n",
    "E_gneutron_det_100 = E_gneutron_det_100.drop(np.where(E_gneutron_det_100==0)[0])\n",
    "E_gphoton_det_100 = E_gphoton_det_100.drop(np.where(E_gphoton_det_100==0)[0])\n",
    "E_gtotal_det_100 = E_gtotal_det_100.drop(np.where(E_gtotal_det_100==0)[0])\n",
    "\n",
    "E_gelectron_det_10g = E_gelectron_det_10g.drop(np.where(E_gelectron_det_10g==0)[0])\n",
    "E_gproton_det_10g = E_gproton_det_10g.drop(np.where(E_gproton_det_10g==0)[0])\n",
    "E_gneutron_det_10g = E_gneutron_det_10g.drop(np.where(E_gneutron_det_10g==0)[0])\n",
    "E_gphoton_det_10g = E_gphoton_det_10g.drop(np.where(E_gphoton_det_10g==0)[0])\n",
    "E_gtotal_det_10g = E_gtotal_det_10g.drop(np.where(E_gtotal_det_10g==0)[0])\n",
    "\n",
    "# Calibrate the energy\n",
    "E_gelectron_cal_100 = np.mean(E_true_100[:len(E_gelectron_det_100)]/np.asarray(E_gelectron_det_100))*E_gelectron_det_100 \n",
    "E_gproton_cal_100 = np.mean(E_true_100[:len(E_gproton_det_100)]/np.asarray(E_gproton_det_100))*E_gproton_det_100\n",
    "E_gneutron_cal_100 = np.mean(E_true_100[:len(E_gneutron_det_100)]/np.asarray(E_gneutron_det_100))*E_gneutron_det_100\n",
    "E_gphoton_cal_100 = np.mean(E_true_100[:len(E_gphoton_det_100)]/np.asarray(E_gphoton_det_100))*E_gphoton_det_100\n",
    "E_gtotal_cal_100 = np.mean(np.asarray(4*list(E_true_100))[:len(E_gtotal_det_100)]/np.asarray(E_gtotal_det_100))*E_gtotal_det_100\n",
    "\n",
    "E_gelectron_cal_10g = np.mean(E_true_10g[:len(E_gelectron_det_10g)]/np.asarray(E_gelectron_det_10g))*E_gelectron_det_10g \n",
    "E_gproton_cal_10g = np.mean(E_true_10g[:len(E_gproton_det_10g)]/np.asarray(E_gproton_det_10g))*E_gproton_det_10g\n",
    "E_gneutron_cal_10g = np.mean(E_true_10g[:len(E_gneutron_det_10g)]/np.asarray(E_gneutron_det_10g))*E_gneutron_det_10g\n",
    "E_gphoton_cal_10g = np.mean(E_true_10g[:len(E_gphoton_det_10g)]/np.asarray(E_gphoton_det_10g))*E_gphoton_det_10g\n",
    "E_gtotal_cal_10g = np.mean(np.asarray(4*list(E_true_10g))[:len(E_gtotal_det_10g)]/np.asarray(E_total_det_10g))*E_gtotal_det_10g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a6c7056",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the weighted differences of the calibrated and true energy\n",
    "electron_gdiff_100 = (np.asarray(E_gelectron_cal_100)-E_true_100[:len(E_gelectron_det_100)])/E_true_100[:len(E_gelectron_det_100)]\n",
    "proton_gdiff_100 = (np.asarray(E_gproton_cal_100)-E_true_100[:len(E_gproton_det_100)])/E_true_100[:len(E_gproton_det_100)]\n",
    "neutron_gdiff_100 = (np.asarray(E_gneutron_cal_100)-E_true_100[:len(E_gneutron_det_100)])/E_true_100[:len(E_gneutron_det_100)]\n",
    "photon_gdiff_100 = (np.asarray(E_gphoton_cal_100)-E_true_100[:len(E_gphoton_det_100)])/E_true_100[:len(E_gphoton_det_100)]\n",
    "total_gdiff_100 = (np.asarray(E_gtotal_cal_100)-np.asarray(4*list(E_true_100))[:len(E_gtotal_det_100)])\\\n",
    "                    /np.asarray(4*list(E_true_100))[:len(E_gtotal_det_100)]\n",
    "\n",
    "electron_gdiff_10g = (np.asarray(E_gelectron_cal_10g)-E_true_10g[:len(E_gelectron_det_10g)])/E_true_10g[:len(E_gelectron_det_10g)]\n",
    "proton_gdiff_10g = (np.asarray(E_gproton_cal_10g)-E_true_10g[:len(E_gproton_det_10g)])/E_true_10g[:len(E_gproton_det_10g)]\n",
    "neutron_gdiff_10g = (np.asarray(E_gneutron_cal_10g)-E_true_10g[:len(E_gneutron_det_10g)])/E_true_10g[:len(E_gneutron_det_10g)]\n",
    "photon_gdiff_10g = (np.asarray(E_gphoton_cal_10g)-E_true_10g[:len(E_gphoton_det_10g)])/E_true_10g[:len(E_gphoton_det_10g)]\n",
    "total_gdiff_10g = (np.asarray(E_gtotal_cal_10g)-np.asarray(4*list(E_true_10g))[:len(E_gtotal_det_10g)])\\\n",
    "                    /np.asarray(4*list(E_true_10g))[:len(E_gtotal_det_10g)]\n",
    "\n",
    "# Calculating the energy resolution\n",
    "electron_gres_100 = np.std(electron_gdiff_100)\n",
    "proton_gres_100 = np.std(proton_gdiff_100)\n",
    "neutron_gres_100 = np.std(neutron_gdiff_100)\n",
    "photon_gres_100 = np.std(photon_gdiff_100)\n",
    "total_gres_100 = np.std(total_gdiff_100)\n",
    "\n",
    "electron_gres_10g = np.std(electron_gdiff_10g)\n",
    "proton_gres_10g = np.std(proton_gdiff_10g)\n",
    "neutron_gres_10g = np.std(neutron_gdiff_10g)\n",
    "photon_gres_10g = np.std(photon_gdiff_10g)\n",
    "total_gres_10g = np.std(total_gdiff_10g)\n",
    "\n",
    "#Plotting the Histograms\n",
    "fig,ax = plt.subplots(2,2,figsize=(12,9))\n",
    "\n",
    "ax[0,0].hist(electron_gdiff_100,label='100 MeV Resolution = {:.4f}'.format(electron_gres_100))\n",
    "ax[0,0].hist(electron_gdiff_10g,label='10 GeV Resolution = {:.4f}'.format(electron_gres_10g),alpha=0.8)\n",
    "ax[0,0].set_title(\"Calibration Energy histogram of Electrons\")\n",
    "ax[0,0].set_xlabel(\"($E_{cal}-E_{true})/E_{true}$\")\n",
    "ax[0,0].set_ylabel(\"Frequency\")\n",
    "ax[0,0].legend()\n",
    "\n",
    "ax[1,0].hist(proton_gdiff_100,label='100 MeV Resolution = {:.4f}'.format(proton_gres_100))\n",
    "ax[1,0].hist(proton_gdiff_10g,label='10 GeV Resolution = {:.4f}'.format(proton_gres_10g),alpha=0.8)\n",
    "ax[1,0].set_title(\"Calibration Energy histogram of Protons\")\n",
    "ax[1,0].set_xlabel(\"($E_{cal}-E_{true})/E_{true}$\")\n",
    "ax[1,0].set_ylabel(\"Frequency\")\n",
    "ax[1,0].legend()\n",
    "\n",
    "ax[0,1].hist(neutron_gdiff_100,label='100 MeV Resolution = {:.4f}'.format(neutron_gres_100))\n",
    "ax[0,1].hist(neutron_gdiff_10g,label='10 GeV Resolution = {:.4f}'.format(neutron_gres_10g),alpha=0.7)\n",
    "ax[0,1].set_title(\"Calibration Energy histogram of Neutrons\")\n",
    "ax[0,1].set_xlabel(\"($E_{cal}-E_{true})/E_{true}$\")\n",
    "ax[0,1].set_ylabel(\"Frequency\")\n",
    "ax[0,1].set_xscale('log')\n",
    "ax[0,1].legend()\n",
    "\n",
    "ax[1,1].hist(photon_gdiff_100,label='100 MeV Resolution = {:.4f}'.format(photon_gres_100))\n",
    "ax[1,1].hist(photon_gdiff_10g,label='10 GeV Resolution = {:.4f}'.format(photon_gres_10g))\n",
    "ax[1,1].set_title(\"Calibration Energy histogram of Photons\")\n",
    "ax[1,1].set_xlabel(\"($E_{cal}-E_{true})/E_{true}$\")\n",
    "ax[1,1].set_ylabel(\"Frequency\")\n",
    "ax[1,1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.hist(total_gdiff_100,label='100 MeV Resolution = {:.4f}'.format(total_gres_100))\n",
    "plt.hist(total_gdiff_10g,label='10 GeV Resolution = {:.4f}'.format(total_gres_10g),alpha=0.8)\n",
    "plt.title(\"Calibration Energy histogram of All Particles\")\n",
    "plt.xlabel(\"($E_{cal}-E_{true})/E_{true}$\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.xscale('log')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faebf147",
   "metadata": {},
   "source": [
    "The energy resolution is not as bad as initially feared. At low energies, the overall resolution has jumped from 9 to 13 which is certainly due to the energy deposited in the target rather than the calorimeters and also due to scattering causing them to fly off and miss the detector altogether. In fact, the proton resolution is still higher at high energies than at low energies meaning the majority of target interactions did not reduce the energy a noticeable amount. This hints at the idea that the majority of interactions are elastic scattering and that the neural network compares between the non-target data and the target data to differentiate the particle. \n",
    "\n",
    "As before, neutrons are still the large contributor to the poor resolution as their neutrality and low cross sections mean they penetrate deeply without depositing much energy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a356106",
   "metadata": {},
   "source": [
    "## Using Tungsten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95bef683",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the data files -- All energies are in MeV\n",
    "\n",
    "# Get rid of column 0 which contains the true energy\n",
    "electrons_100mev_Wtarget = pd.read_csv( \"Electrons_100mev_Wtarget.csv\", comment=\"#\",usecols=[1,2,3,4,5,6],\\\n",
    "names=[ \"HomogeneousWT\", \"Layer 1WT\",\"Layer 2WT\", \"Layer 3WT\",\"Layer 4WT\",\"Layer 5WT\" ] )\n",
    "\n",
    "protons_100mev_Wtarget = pd.read_csv( \"Protons_100mev_Wtarget.csv\", comment=\"#\",usecols=[1,2,3,4,5,6],\\\n",
    "names=[ \"HomogeneousWT\", \"Layer 1WT\",\"Layer 2WT\", \"Layer 3WT\",\"Layer 4WT\",\"Layer 5WT\" ] )\n",
    "\n",
    "neutrons_100mev_Wtarget = pd.read_csv( \"Neutrons_100mev_Wtarget.csv\", comment=\"#\",usecols=[1,2,3,4,5,6],\\\n",
    "names=[ \"HomogeneousWT\", \"Layer 1WT\",\"Layer 2WT\", \"Layer 3WT\",\"Layer 4WT\",\"Layer 5WT\" ] )\n",
    "\n",
    "photons_100mev_Wtarget = pd.read_csv( \"Photons_100mev_Wtarget.csv\", comment=\"#\",usecols=[1,2,3,4,5,6],\\\n",
    "names=[ \"HomogeneousWT\", \"Layer 1WT\",\"Layer 2WT\", \"Layer 3WT\",\"Layer 4WT\",\"Layer 5WT\" ] )\n",
    "\n",
    "electrons_10gev_Wtarget = pd.read_csv( \"Electrons_10gev_Wtarget.csv\", comment=\"#\",usecols=[1,2,3,4,5,6],\\\n",
    "names=[ \"HomogeneousWT\", \"Layer 1WT\",\"Layer 2WT\", \"Layer 3WT\",\"Layer 4WT\",\"Layer 5WT\" ] )\n",
    "\n",
    "protons_10gev_Wtarget = pd.read_csv( \"Protons_10gev_Wtarget.csv\", comment=\"#\",usecols=[1,2,3,4,5,6],\\\n",
    "names=[ \"HomogeneousWT\", \"Layer 1WT\",\"Layer 2WT\", \"Layer 3WT\",\"Layer 4WT\",\"Layer 5WT\" ] )\n",
    "\n",
    "neutrons_10gev_Wtarget = pd.read_csv( \"Neutrons_10gev_Wtarget.csv\", comment=\"#\",usecols=[1,2,3,4,5,6],\\\n",
    "names=[ \"HomogeneousWT\", \"Layer 1WT\",\"Layer 2WT\", \"Layer 3WT\",\"Layer 4WT\",\"Layer 5WT\" ] )\n",
    "\n",
    "photons_10gev_Wtarget = pd.read_csv( \"Photons_10gev_Wtarget.csv\", comment=\"#\",usecols=[1,2,3,4,5,6],\\\n",
    "names=[ \"HomogeneousWT\", \"Layer 1WT\",\"Layer 2WT\", \"Layer 3WT\",\"Layer 4WT\",\"Layer 5WT\" ] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b729679",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append the data alongside the non-target data.\n",
    "\n",
    "tungsten_electrons_100mev = pd.concat([electrons_100mev,electrons_100mev_Wtarget],axis=1)\n",
    "tungsten_protons_100mev = pd.concat([protons_100mev,protons_100mev_Wtarget],axis=1)\n",
    "tungsten_neutrons_100mev = pd.concat([neutrons_100mev,neutrons_100mev_Wtarget],axis=1)\n",
    "tungsten_photons_100mev = pd.concat([photons_100mev,photons_100mev_Wtarget],axis=1)\n",
    "tungsten_electrons_10gev = pd.concat([electrons_10gev,electrons_10gev_Wtarget],axis=1)\n",
    "tungsten_protons_10gev = pd.concat([protons_10gev,protons_10gev_Wtarget],axis=1)\n",
    "tungsten_neutrons_10gev = pd.concat([neutrons_10gev,neutrons_10gev_Wtarget],axis=1)\n",
    "tungsten_photons_10gev = pd.concat([photons_10gev,photons_10gev_Wtarget],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ed5b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append all the dataframes together by energy and shuffle to create our dataset\n",
    "\n",
    "#Appending\n",
    "tungsten_particles_100mev = pd.concat([tungsten_electrons_100mev,tungsten_protons_100mev,tungsten_neutrons_100mev,\\\n",
    "                                  tungsten_photons_100mev],ignore_index=True)\n",
    "tungsten_particles_10gev = pd.concat([tungsten_electrons_10gev,tungsten_protons_10gev,tungsten_neutrons_10gev,\\\n",
    "                                 tungsten_photons_10gev],ignore_index=True)\n",
    "\n",
    "# Shuffling\n",
    "tungsten_particles_100mev = tungsten_particles_100mev.sample(frac=1).reset_index(drop=True)\n",
    "tungsten_particles_10gev = tungsten_particles_10gev.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a598cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check to see if our data frame looks good\n",
    "tungsten_particles_100mev\n",
    "# It looks great!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b104cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create our testing and training datasets\n",
    "\n",
    "Wt_y_100 = tungsten_particles_100mev[[\"Particle ID\"]]\n",
    "Wt_x_100 = tungsten_particles_100mev[[\"Homogeneous\",\"Layer 1\",\"Layer 2\", \"Layer 3\",\"Layer 4\",\"Layer 5\",\\\n",
    "                           \"HomogeneousWT\", \"Layer 1WT\",\"Layer 2WT\", \"Layer 3WT\",\"Layer 4WT\",\"Layer 5WT\"]]\n",
    "\n",
    "Wt_y_10 = tungsten_particles_10gev[[\"Particle ID\"]]\n",
    "Wt_x_10 = tungsten_particles_10gev[[\"Homogeneous\",\"Layer 1\",\"Layer 2\", \"Layer 3\",\"Layer 4\",\"Layer 5\",\\\n",
    "                         \"HomogeneousWT\", \"Layer 1WT\",\"Layer 2WT\", \"Layer 3WT\",\"Layer 4WT\",\"Layer 5WT\"]]\n",
    "\n",
    "Wt_x100m_train, Wt_x100m_test, Wt_y100m_train, Wt_y100m_test = train_test_split(Wt_x_100,Wt_y_100, test_size=0.3,\n",
    "                                                        random_state=1) # 70% training and 30% test\n",
    "\n",
    "Wt_x10g_train, Wt_x10g_test, Wt_y10g_train, Wt_y10g_test = train_test_split(Wt_x_10,Wt_y_10, test_size=0.3,\n",
    "                                                        random_state=1) # 70% training and 30% test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e92e74dd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Again, use the same model structure\n",
    "model_Wt_100m = model(np.shape(Wt_x100m_train)[1])\n",
    "model_Wt_10g = model(np.shape(Wt_x10g_train)[1])\n",
    "\n",
    "model_Wt_100m.summary()\n",
    "# This looks the same so all is well "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "050e1051",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a large batch size of 512 to remove the fluctuations and 80 epochs to remove overtraining\n",
    "history_Wt_100m = model_Wt_100m.fit(x=Wt_x100m_train,y=Wt_y100m_train,batch_size=512,\\\n",
    "                                              epochs=80,validation_split=0.2,shuffle=True)\n",
    "\n",
    "\n",
    "# Use batch size of 128 and with 100 epochs\n",
    "history_Wt_10g = model_Wt_10g.fit(x=Wt_x10g_train,y=Wt_y10g_train,batch_size=128,\\\n",
    "                                              epochs=100,validation_split=0.2,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c6f34f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting the loss curves\n",
    "\n",
    "fig,ax = plt.subplots(2,2,figsize=(12,9))\n",
    "ax[0,0].plot(history_Wt_100m.history['loss'],label='Loss')\n",
    "ax[0,0].plot(history_Wt_100m.history['val_loss'],label='Validation Loss')\n",
    "ax[0,0].set_title(\"Loss Change over Time (100 MeV)\")\n",
    "ax[0,0].set_xlabel(\"Epoch\")\n",
    "ax[0,0].set_ylabel(\"Loss\")\n",
    "ax[0,0].legend()\n",
    "\n",
    "ax[1,0].plot(history_Wt_100m.history['accuracy'],label='Accuracy')\n",
    "ax[1,0].plot(history_Wt_100m.history['val_accuracy'],label='Validation Accuracy')\n",
    "ax[1,0].set_title(\"Accuracy Change over Time (100 MeV)\")\n",
    "ax[1,0].set_xlabel(\"Epoch\")\n",
    "ax[1,0].set_ylabel(\"Accuracy\")\n",
    "ax[1,0].legend()\n",
    "\n",
    "ax[0,1].plot(history_Wt_10g.history['loss'],label='Loss')\n",
    "ax[0,1].plot(history_Wt_10g.history['val_loss'],label='Validation Loss')\n",
    "ax[0,1].set_title(\"Loss Change over Time (10 GeV)\")\n",
    "ax[0,1].set_xlabel(\"Epoch\")\n",
    "ax[0,1].set_ylabel(\"Loss\")\n",
    "ax[0,1].legend()\n",
    "\n",
    "ax[1,1].plot(history_Wt_10g.history['accuracy'],label='Accuracy')\n",
    "ax[1,1].plot(history_Wt_10g.history['val_accuracy'],label='Validation Accuracy')\n",
    "ax[1,1].set_title(\"Accuracy Change over Time (10 GeV)\")\n",
    "ax[1,1].set_xlabel(\"Epoch\")\n",
    "ax[1,1].set_ylabel(\"Accuracy\")\n",
    "ax[1,1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5a5fd4",
   "metadata": {},
   "source": [
    "After last time's success, it is disappointing to see that these loss curves are worse than for graphite. These ones are more comparable to the original model except it does have a 5% higher accuracy at 10 GeV than for the original. The reason for this is probably because tungsten is a relatively heavy element (A=74) compared to carbon (A=12) meaning it is more absorbent. Therefore, it is possible that many of the particles were absorbed entirely into the target at low energies. However, at 10 GeV, it appears many did interact and still deposited energy into the detector so it does have a greater accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0aa01e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the model on the testing data\n",
    "prediction_Wt_100m = np.argmax(model_Wt_100m.predict(Wt_x100m_test),axis=1)\n",
    "prediction_Wt_10g = np.argmax(model_Wt_10g.predict(Wt_x10g_test),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ea155c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "fig,ax = plt.subplots(1,2,figsize=(12,5))\n",
    "#Define the confusion matrix and normalise it so that each row/column sums to 1\n",
    "Wt_cmat100m = confusion_matrix(Wt_y100m_test,prediction_Wt_100m,normalize='true')\n",
    "Wt_cmat10g = confusion_matrix(Wt_y10g_test,prediction_Wt_10g,normalize='true')\n",
    "#dispay the matrix and redefine the display labels to show the particle type\n",
    "Wt_cmatplot100m = ConfusionMatrixDisplay(confusion_matrix=Wt_cmat100m,display_labels=['electron','proton','neutron','photon'])\n",
    "Wt_cmatplot10g = ConfusionMatrixDisplay(confusion_matrix=Wt_cmat10g,display_labels=['electron','proton','neutron','photon'])\n",
    "\n",
    "Wt_cmatplot100m.plot(ax=ax[0])\n",
    "ax[0].set_title(\"100 MeV\")\n",
    "Wt_cmatplot10g.plot(ax=ax[1])\n",
    "ax[1].set_title(\"10 GeV\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2080cb8b",
   "metadata": {},
   "source": [
    "As expected from the loss curves, the confusion matrices are similar to the original ones. At low energies, the tungsten does not induce enough showering scattering reactions to provide any different data to the original. In fact, looking at the data, it is seen that a lot of the values are 0 for the energy -- for all particles. Thus, it appears that at 100 MeV, no additional useful data is being added to the neural network thus it is giving a very similar performance as the original one. The tungsten block is absorbing too many particles.\n",
    "\n",
    "The 10 GeV matrix is slightly more positive as it is better at differentiating between neutrons and even photons. Presumably, at these high energies, some of the uncharged neutrons and photons pass straight through the tungsten shield and are able to deposit their energies into the calorimeter. Therefore, any additional data will be interpreted as coming from either neutrons or photons -- the difference between those two being that the photons deposit their energy in the homogeneous calorimeter and the neutrons in the sampling calorimeter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ffd6f18",
   "metadata": {},
   "source": [
    "## Energy Calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd166f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the energy resolution for each particle separately and then all of them together\n",
    "\n",
    "# Getting the data required \n",
    "E_true_100 = electrons_100mev['True Energy'] # Same for every particle\n",
    "E_true_10g = electrons_10gev['True Energy']\n",
    "# Total detected energy\n",
    "E_welectron_det_100 = tungsten_electrons_100mev[[\"HomogeneousWT\",\"Layer 1WT\",\"Layer 2WT\", \"Layer 3WT\",\"Layer 4WT\",\"Layer 5WT\"]].sum(axis=1)\n",
    "E_wproton_det_100 = tungsten_protons_100mev[[\"HomogeneousWT\",\"Layer 1WT\",\"Layer 2WT\", \"Layer 3WT\",\"Layer 4WT\",\"Layer 5WT\"]].sum(axis=1)\n",
    "E_wneutron_det_100 = tungsten_neutrons_100mev[[\"HomogeneousWT\",\"Layer 1WT\",\"Layer 2WT\", \"Layer 3WT\",\"Layer 4WT\",\"Layer 5WT\"]].sum(axis=1)\n",
    "E_wphoton_det_100 = tungsten_photons_100mev[[\"HomogeneousWT\",\"Layer 1WT\",\"Layer 2WT\", \"Layer 3WT\",\"Layer 4WT\",\"Layer 5WT\"]].sum(axis=1)\n",
    "E_wtotal_det_100 = pd.concat([E_welectron_det_100,E_wproton_det_100,E_wneutron_det_100,E_wphoton_det_100],\\\n",
    "                           ignore_index=True)\n",
    "\n",
    "E_welectron_det_10g = tungsten_electrons_10gev[[\"Homogeneous\",\"Layer 1\",\"Layer 2\", \"Layer 3\",\"Layer 4\",\"Layer 5\"]].sum(axis=1)\n",
    "E_wproton_det_10g = tungsten_protons_10gev[[\"Homogeneous\",\"Layer 1\",\"Layer 2\", \"Layer 3\",\"Layer 4\",\"Layer 5\"]].sum(axis=1)\n",
    "E_wneutron_det_10g = tungsten_neutrons_10gev[[\"Homogeneous\",\"Layer 1\",\"Layer 2\", \"Layer 3\",\"Layer 4\",\"Layer 5\"]].sum(axis=1)\n",
    "E_wphoton_det_10g = tungsten_photons_10gev[[\"Homogeneous\",\"Layer 1\",\"Layer 2\", \"Layer 3\",\"Layer 4\",\"Layer 5\"]].sum(axis=1)\n",
    "E_wtotal_det_10g = pd.concat([E_welectron_det_10g,E_wproton_det_10g,E_wneutron_det_10g,E_wphoton_det_10g],\\\n",
    "                           ignore_index=True)\n",
    "\n",
    "# Remove zero values to avoid divide by 0 errors\n",
    "E_welectron_det_100 = E_welectron_det_100.drop(np.where(E_welectron_det_100==0)[0])\n",
    "E_wproton_det_100 = E_wproton_det_100.drop(np.where(E_wproton_det_100==0)[0])\n",
    "E_wneutron_det_100 = E_wneutron_det_100.drop(np.where(E_wneutron_det_100==0)[0])\n",
    "E_wphoton_det_100 = E_wphoton_det_100.drop(np.where(E_wphoton_det_100==0)[0])\n",
    "E_wtotal_det_100 = E_wtotal_det_100.drop(np.where(E_wtotal_det_100==0)[0])\n",
    "\n",
    "E_welectron_det_10g = E_welectron_det_10g.drop(np.where(E_welectron_det_10g==0)[0])\n",
    "E_wproton_det_10g = E_wproton_det_10g.drop(np.where(E_wproton_det_10g==0)[0])\n",
    "E_wneutron_det_10g = E_wneutron_det_10g.drop(np.where(E_wneutron_det_10g==0)[0])\n",
    "E_wphoton_det_10g = E_wphoton_det_10g.drop(np.where(E_wphoton_det_10g==0)[0])\n",
    "E_wtotal_det_10g = E_wtotal_det_10g.drop(np.where(E_wtotal_det_10g==0)[0])\n",
    "\n",
    "# Calibrate the energy\n",
    "E_welectron_cal_100 = np.mean(E_true_100[:len(E_welectron_det_100)]/np.asarray(E_welectron_det_100))*E_welectron_det_100 \n",
    "E_wproton_cal_100 = np.mean(E_true_100[:len(E_wproton_det_100)]/np.asarray(E_wproton_det_100))*E_wproton_det_100\n",
    "E_wneutron_cal_100 = np.mean(E_true_100[:len(E_wneutron_det_100)]/np.asarray(E_wneutron_det_100))*E_wneutron_det_100\n",
    "E_wphoton_cal_100 = np.mean(E_true_100[:len(E_wphoton_det_100)]/np.asarray(E_wphoton_det_100))*E_wphoton_det_100\n",
    "E_wtotal_cal_100 = np.mean(np.asarray(4*list(E_true_100))[:len(E_wtotal_det_100)]/np.asarray(E_wtotal_det_100))*E_wtotal_det_100\n",
    "\n",
    "E_welectron_cal_10g = np.mean(E_true_10g[:len(E_welectron_det_10g)]/np.asarray(E_welectron_det_10g))*E_welectron_det_10g \n",
    "E_wproton_cal_10g = np.mean(E_true_10g[:len(E_wproton_det_10g)]/np.asarray(E_wproton_det_10g))*E_wproton_det_10g\n",
    "E_wneutron_cal_10g = np.mean(E_true_10g[:len(E_wneutron_det_10g)]/np.asarray(E_wneutron_det_10g))*E_wneutron_det_10g\n",
    "E_wphoton_cal_10g = np.mean(E_true_10g[:len(E_wphoton_det_10g)]/np.asarray(E_wphoton_det_10g))*E_wphoton_det_10g\n",
    "E_wtotal_cal_10g = np.mean(np.asarray(4*list(E_true_10g))[:len(E_gtotal_det_10g)]/np.asarray(E_wtotal_det_10g))*E_wtotal_det_10g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e609bd7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Calculating the weighted differences of the calibrated and true energy\n",
    "electron_wdiff_100 = (np.asarray(E_welectron_cal_100)-E_true_100[:len(E_welectron_det_100)])/E_true_100[:len(E_welectron_det_100)]\n",
    "proton_wdiff_100 = (np.asarray(E_wproton_cal_100)-E_true_100[:len(E_wproton_det_100)])/E_true_100[:len(E_wproton_det_100)]\n",
    "neutron_wdiff_100 = (np.asarray(E_wneutron_cal_100)-E_true_100[:len(E_wneutron_det_100)])/E_true_100[:len(E_wneutron_det_100)]\n",
    "photon_wdiff_100 = (np.asarray(E_wphoton_cal_100)-E_true_100[:len(E_wphoton_det_100)])/E_true_100[:len(E_wphoton_det_100)]\n",
    "total_wdiff_100 = (np.asarray(E_wtotal_cal_100)-np.asarray(4*list(E_true_100))[:len(E_wtotal_det_100)])\\\n",
    "                    /np.asarray(4*list(E_true_100))[:len(E_wtotal_det_100)]\n",
    "\n",
    "electron_wdiff_10g = (np.asarray(E_welectron_cal_10g)-E_true_10g[:len(E_welectron_det_10g)])/E_true_10g[:len(E_welectron_det_10g)]\n",
    "proton_wdiff_10g = (np.asarray(E_wproton_cal_10g)-E_true_10g[:len(E_wproton_det_10g)])/E_true_10g[:len(E_wproton_det_10g)]\n",
    "neutron_wdiff_10g = (np.asarray(E_wneutron_cal_10g)-E_true_10g[:len(E_wneutron_det_10g)])/E_true_10g[:len(E_wneutron_det_10g)]\n",
    "photon_wdiff_10g = (np.asarray(E_wphoton_cal_10g)-E_true_10g[:len(E_wphoton_det_10g)])/E_true_10g[:len(E_wphoton_det_10g)]\n",
    "total_wdiff_10g = (np.asarray(E_wtotal_cal_10g)-np.asarray(4*list(E_true_10g))[:len(E_wtotal_det_10g)])\\\n",
    "                    /np.asarray(4*list(E_true_10g))[:len(E_wtotal_det_10g)]\n",
    "\n",
    "# Calculating the energy resolution\n",
    "electron_wres_100 = np.std(electron_wdiff_100)\n",
    "proton_wres_100 = np.std(proton_wdiff_100)\n",
    "neutron_wres_100 = np.std(neutron_wdiff_100)\n",
    "photon_wres_100 = np.std(photon_wdiff_100)\n",
    "total_wres_100 = np.std(total_wdiff_100)\n",
    "\n",
    "electron_wres_10g = np.std(electron_wdiff_10g)\n",
    "proton_wres_10g = np.std(proton_wdiff_10g)\n",
    "neutron_wres_10g = np.std(neutron_wdiff_10g)\n",
    "photon_wres_10g = np.std(photon_wdiff_10g)\n",
    "total_wres_10g = np.std(total_wdiff_10g)\n",
    "\n",
    "#Plotting the Histograms\n",
    "fig,ax = plt.subplots(2,2,figsize=(12,9))\n",
    "\n",
    "ax[0,0].hist(electron_wdiff_100,label='100 MeV Resolution = {:.4f}'.format(electron_wres_100))\n",
    "ax[0,0].hist(electron_wdiff_10g,label='10 GeV Resolution = {:.4f}'.format(electron_wres_10g))\n",
    "ax[0,0].set_title(\"Calibration Energy histogram of Electrons\")\n",
    "ax[0,0].set_xlabel(\"($E_{cal}-E_{true})/E_{true}$\")\n",
    "ax[0,0].set_ylabel(\"Frequency\")\n",
    "ax[0,0].legend()\n",
    "\n",
    "ax[1,0].hist(proton_wdiff_100,label='100 MeV Resolution = {:.4f}'.format(proton_wres_100))\n",
    "ax[1,0].hist(proton_wdiff_10g,label='10 GeV Resolution = {:.4f}'.format(proton_wres_10g),alpha=0.8)\n",
    "ax[1,0].set_title(\"Calibration Energy histogram of Protons\")\n",
    "ax[1,0].set_xlabel(\"($E_{cal}-E_{true})/E_{true}$\")\n",
    "ax[1,0].set_ylabel(\"Frequency\")\n",
    "ax[1,0].set_xscale('log')\n",
    "ax[1,0].legend()\n",
    "\n",
    "ax[0,1].hist(neutron_wdiff_100,label='100 MeV Resolution = {:.4f}'.format(neutron_wres_100))\n",
    "ax[0,1].hist(neutron_wdiff_10g,label='10 GeV Resolution = {:.4f}'.format(neutron_wres_10g),alpha=0.8)\n",
    "ax[0,1].set_title(\"Calibration Energy histogram of Neutrons\")\n",
    "ax[0,1].set_xlabel(\"($E_{cal}-E_{true})/E_{true}$\")\n",
    "ax[0,1].set_ylabel(\"Frequency\")\n",
    "ax[0,1].set_xscale('log')\n",
    "ax[0,1].legend()\n",
    "\n",
    "ax[1,1].hist(photon_wdiff_100,label='100 MeV Resolution = {:.4f}'.format(photon_wres_100))\n",
    "ax[1,1].hist(photon_wdiff_10g,label='10 GeV Resolution = {:.4f}'.format(photon_wres_10g))\n",
    "ax[1,1].set_title(\"Calibration Energy histogram of Photons\")\n",
    "ax[1,1].set_xlabel(\"($E_{cal}-E_{true})/E_{true}$\")\n",
    "ax[1,1].set_ylabel(\"Frequency\")\n",
    "ax[1,1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.hist(total_wdiff_100,label='100 MeV Resolution = {:.4f}'.format(total_wres_100))\n",
    "plt.hist(total_wdiff_10g,label='10 GeV Resolution = {:.4f}'.format(total_wres_10g),alpha=0.8)\n",
    "plt.title(\"Calibration Energy histogram of All Particles\")\n",
    "plt.xlabel(\"($E_{cal}-E_{true})/E_{true}$\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.xscale('log')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54dee2e4",
   "metadata": {},
   "source": [
    "The energy resolution for the high energy particles remains largely unchanged as they are still able to penetrate the target whereas the energy resolution for 100 MeV is significantly higher. This is due to the fact that the tungsten absorbs most of the particles at that energy so many of them deposit no energy at all! As a result, we have the topsy-turvy scenario of the 10 GeV particles having a higher energy resolution than the 100 MeV particles. For the neutrons, however, it still doesn't hold as not even tungsten is dense enough to stop them from going through everything. Because they influence the energy resolution the most, the overall energy resolution is still better for 100 MeV particles -- but only by a factor of ~7."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d6dda1",
   "metadata": {},
   "source": [
    "## Using Beryllium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4334f9a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the data files -- All energies are in MeV\n",
    "\n",
    "# Get rid of column 0 which contains the true energy\n",
    "electrons_100mev_Betarget = pd.read_csv( \"Electrons_100mev_Betarget.csv\", comment=\"#\",usecols=[1,2,3,4,5,6],\\\n",
    "names=[ \"HomogeneousBeT\", \"Layer 1BeT\",\"Layer 2BeT\", \"Layer 3BeT\",\"Layer 4BeT\",\"Layer 5BeT\" ] )\n",
    "\n",
    "protons_100mev_Betarget = pd.read_csv( \"Protons_100mev_Betarget.csv\", comment=\"#\",usecols=[1,2,3,4,5,6],\\\n",
    "names=[ \"HomogeneousBeT\", \"Layer 1BeT\",\"Layer 2BeT\", \"Layer 3BeT\",\"Layer 4BeT\",\"Layer 5BeT\" ] )\n",
    "\n",
    "neutrons_100mev_Betarget = pd.read_csv( \"Neutrons_100mev_Betarget.csv\", comment=\"#\",usecols=[1,2,3,4,5,6],\\\n",
    "names=[ \"HomogeneousBeT\", \"Layer 1BeT\",\"Layer 2BeT\", \"Layer 3BeT\",\"Layer 4BeT\",\"Layer 5BeT\" ] )\n",
    "\n",
    "photons_100mev_Betarget = pd.read_csv( \"Photons_100mev_Betarget.csv\", comment=\"#\",usecols=[1,2,3,4,5,6],\\\n",
    "names=[ \"HomogeneousBeT\", \"Layer 1BeT\",\"Layer 2BeT\", \"Layer 3BeT\",\"Layer 4BeT\",\"Layer 5BeT\" ] )\n",
    "\n",
    "electrons_10gev_Betarget = pd.read_csv( \"Electrons_10gev_Betarget.csv\", comment=\"#\",usecols=[1,2,3,4,5,6],\\\n",
    "names=[ \"HomogeneousBeT\", \"Layer 1BeT\",\"Layer 2BeT\", \"Layer 3BeT\",\"Layer 4BeT\",\"Layer 5BeT\" ] )\n",
    "\n",
    "protons_10gev_Betarget = pd.read_csv( \"Protons_10gev_Betarget.csv\", comment=\"#\",usecols=[1,2,3,4,5,6],\\\n",
    "names=[ \"HomogeneousBeT\", \"Layer 1BeT\",\"Layer 2BeT\", \"Layer 3BeT\",\"Layer 4BeT\",\"Layer 5BeT\" ] )\n",
    "\n",
    "neutrons_10gev_Betarget = pd.read_csv( \"Neutrons_10gev_Betarget.csv\", comment=\"#\",usecols=[1,2,3,4,5,6],\\\n",
    "names=[ \"HomogeneousBeT\", \"Layer 1BeT\",\"Layer 2BeT\", \"Layer 3BeT\",\"Layer 4BeT\",\"Layer 5BeT\" ] )\n",
    "\n",
    "photons_10gev_Betarget = pd.read_csv( \"Photons_10gev_Betarget.csv\", comment=\"#\",usecols=[1,2,3,4,5,6],\\\n",
    "names=[ \"HomogeneousBeT\", \"Layer 1BeT\",\"Layer 2BeT\", \"Layer 3BeT\",\"Layer 4BeT\",\"Layer 5BeT\" ] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14249a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append the data alongside the non-target data.\n",
    "\n",
    "beryllium_electrons_100mev = pd.concat([electrons_100mev,electrons_100mev_Betarget],axis=1)\n",
    "beryllium_protons_100mev = pd.concat([protons_100mev,protons_100mev_Betarget],axis=1)\n",
    "beryllium_neutrons_100mev = pd.concat([neutrons_100mev,neutrons_100mev_Betarget],axis=1)\n",
    "beryllium_photons_100mev = pd.concat([photons_100mev,photons_100mev_Betarget],axis=1)\n",
    "beryllium_electrons_10gev = pd.concat([electrons_10gev,electrons_10gev_Betarget],axis=1)\n",
    "beryllium_protons_10gev = pd.concat([protons_10gev,protons_10gev_Betarget],axis=1)\n",
    "beryllium_neutrons_10gev = pd.concat([neutrons_10gev,neutrons_10gev_Betarget],axis=1)\n",
    "beryllium_photons_10gev = pd.concat([photons_10gev,photons_10gev_Betarget],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "160caef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append all the dataframes together by energy and shuffle to create our dataset\n",
    "\n",
    "#Appending\n",
    "beryllium_particles_100mev = pd.concat([beryllium_electrons_100mev,beryllium_protons_100mev,beryllium_neutrons_100mev,\\\n",
    "                                  beryllium_photons_100mev],ignore_index=True)\n",
    "beryllium_particles_10gev = pd.concat([beryllium_electrons_10gev,beryllium_protons_10gev,beryllium_neutrons_10gev,\\\n",
    "                                 beryllium_photons_10gev],ignore_index=True)\n",
    "\n",
    "#Shuffling\n",
    "beryllium_particles_100mev = beryllium_particles_100mev.sample(frac=1).reset_index(drop=True)\n",
    "beryllium_particles_10gev = beryllium_particles_10gev.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb6955c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check to see if our data frame looks good\n",
    "beryllium_particles_100mev\n",
    "# It looks great!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7797fed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create our testing and training datasets\n",
    "\n",
    "Bet_y_100 = beryllium_particles_100mev[[\"Particle ID\"]]\n",
    "Bet_x_100 = beryllium_particles_100mev[[\"Homogeneous\",\"Layer 1\",\"Layer 2\", \"Layer 3\",\"Layer 4\",\"Layer 5\",\\\n",
    "                           \"HomogeneousBeT\", \"Layer 1BeT\",\"Layer 2BeT\", \"Layer 3BeT\",\"Layer 4BeT\",\"Layer 5BeT\"]]\n",
    "\n",
    "Bet_y_10 = beryllium_particles_10gev[[\"Particle ID\"]]\n",
    "Bet_x_10 = beryllium_particles_10gev[[\"Homogeneous\",\"Layer 1\",\"Layer 2\", \"Layer 3\",\"Layer 4\",\"Layer 5\",\\\n",
    "                         \"HomogeneousBeT\", \"Layer 1BeT\",\"Layer 2BeT\", \"Layer 3BeT\",\"Layer 4BeT\",\"Layer 5BeT\"]]\n",
    "\n",
    "Bet_x100m_train, Bet_x100m_test, Bet_y100m_train, Bet_y100m_test = train_test_split(Bet_x_100,Bet_y_100, test_size=0.3,\n",
    "                                                        random_state=1) # 70% training and 30% test\n",
    "\n",
    "Bet_x10g_train, Bet_x10g_test, Bet_y10g_train, Bet_y10g_test = train_test_split(Bet_x_10,Bet_y_10, test_size=0.3,\n",
    "                                                        random_state=1) # 70% training and 30% test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef2da62",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Use the same model structure\n",
    "model_Bet_100m = model(np.shape(Bet_x100m_train)[1])\n",
    "model_Bet_10g = model(np.shape(Bet_x10g_train)[1])\n",
    "\n",
    "model_Bet_100m.summary()\n",
    "# This looks the same so all is well "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7275b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train for 100 epochs with 128 batch size and 20% validation split\n",
    "history_Bet_100m = model_Bet_100m.fit(x=Bet_x100m_train,y=Bet_y100m_train,batch_size=128,\\\n",
    "                                              epochs=100,validation_split=0.2,shuffle=True)\n",
    "\n",
    "# Train for 100 epochs with 128 batch size and 20% validation split\n",
    "history_Bet_10g = model_Bet_10g.fit(x=Bet_x10g_train,y=Bet_y10g_train,batch_size=128,\\\n",
    "                                              epochs=100,validation_split=0.2,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69dc7056",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting the loss curves\n",
    "\n",
    "fig,ax = plt.subplots(2,2,figsize=(12,9))\n",
    "ax[0,0].plot(history_Bet_100m.history['loss'],label='Loss')\n",
    "ax[0,0].plot(history_Bet_100m.history['val_loss'],label='Validation Loss')\n",
    "ax[0,0].set_title(\"Loss Change over Time (100 MeV)\")\n",
    "ax[0,0].set_xlabel(\"Epoch\")\n",
    "ax[0,0].set_ylabel(\"Loss\")\n",
    "ax[0,0].legend()\n",
    "\n",
    "ax[1,0].plot(history_Bet_100m.history['accuracy'],label='Accuracy')\n",
    "ax[1,0].plot(history_Bet_100m.history['val_accuracy'],label='Validation Accuracy')\n",
    "ax[1,0].set_title(\"Accuracy Change over Time (100 MeV)\")\n",
    "ax[1,0].set_xlabel(\"Epoch\")\n",
    "ax[1,0].set_ylabel(\"Accuracy\")\n",
    "ax[1,0].legend()\n",
    "\n",
    "ax[0,1].plot(history_Bet_10g.history['loss'],label='Loss')\n",
    "ax[0,1].plot(history_Bet_10g.history['val_loss'],label='Validation Loss')\n",
    "ax[0,1].set_title(\"Loss Change over Time (10 GeV)\")\n",
    "ax[0,1].set_xlabel(\"Epoch\")\n",
    "ax[0,1].set_ylabel(\"Loss\")\n",
    "ax[0,1].legend()\n",
    "\n",
    "ax[1,1].plot(history_Bet_10g.history['accuracy'],label='Accuracy')\n",
    "ax[1,1].plot(history_Bet_10g.history['val_accuracy'],label='Validation Accuracy')\n",
    "ax[1,1].set_title(\"Accuracy Change over Time (10 GeV)\")\n",
    "ax[1,1].set_xlabel(\"Epoch\")\n",
    "ax[1,1].set_ylabel(\"Accuracy\")\n",
    "ax[1,1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05296b0f",
   "metadata": {},
   "source": [
    "From these loss curves, like graphite, it is possible to see that the beryllium target is most beneficial for distinguishing low energy particles. Despite not having an accuracy as high as graphite, it is still superior to the original set-up and the lines are pretty stable and show no signs of overtraining meaning the batch size and epoch number is justified.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e7e987",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the model on the testing datasets\n",
    "prediction_Bet_100m = np.argmax(model_Bet_100m.predict(Bet_x100m_test),axis=1)\n",
    "prediction_Bet_10g = np.argmax(model_Bet_10g.predict(Bet_x10g_test),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b0ca28",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "fig,ax = plt.subplots(1,2,figsize=(12,5))\n",
    "#Define the confusion matrix and normalise it so that each row/column sums to 1\n",
    "Bet_cmat100m = confusion_matrix(Bet_y100m_test,prediction_Bet_100m,normalize='true')\n",
    "Bet_cmat10g = confusion_matrix(Bet_y10g_test,prediction_Bet_10g,normalize='true')\n",
    "#dispay the matrix and redefine the display labels to show the particle type\n",
    "Bet_cmatplot100m = ConfusionMatrixDisplay(confusion_matrix=Bet_cmat100m,display_labels=['electron','proton','neutron','photon'])\n",
    "Bet_cmatplot10g = ConfusionMatrixDisplay(confusion_matrix=Bet_cmat10g,display_labels=['electron','proton','neutron','photon'])\n",
    "\n",
    "Bet_cmatplot100m.plot(ax=ax[0])\n",
    "ax[0].set_title(\"100 MeV\")\n",
    "Bet_cmatplot10g.plot(ax=ax[1])\n",
    "ax[1].set_title(\"10 GeV\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ad91f8",
   "metadata": {},
   "source": [
    "These confusion matrices are comparable to those of graphite as the distinction between hadrons and elementary particles in enhanced compared to the original matrix and the distinction amongst the hadrons themselves is also increased by a similar amount. The only difference between the graphite matrices and these ones large enough worth mentioning is the fact that the photon is detected worse here than with graphite. It is unclear what the reason is but the confusion between photons and electrons is much higher this time. It could be the electronic/atomic structure of beryllium that causes the photons to pair produce positron-electron pairs that then enter the calorimeter disguised as electrons. Because they have lost a similar amount of energy to the electrons, it is possible that they deposit very similar amounts of energy in the homogeneous calorimeter. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0973683",
   "metadata": {},
   "source": [
    "## Energy Resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87dc7c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the energy resolution for each particle separately and then all of them together\n",
    "\n",
    "# Getting the data required \n",
    "E_true_100 = electrons_100mev['True Energy'] # Same for every particle\n",
    "E_true_10g = electrons_10gev['True Energy']\n",
    "# Total detected energy\n",
    "E_Beelectron_det_100 = beryllium_electrons_100mev[[\"HomogeneousBeT\",\"Layer 1BeT\",\"Layer 2BeT\", \"Layer 3BeT\",\"Layer 4BeT\",\"Layer 5BeT\"]].sum(axis=1)\n",
    "E_Beproton_det_100 = beryllium_protons_100mev[[\"HomogeneousBeT\",\"Layer 1BeT\",\"Layer 2BeT\", \"Layer 3BeT\",\"Layer 4BeT\",\"Layer 5BeT\"]].sum(axis=1)\n",
    "E_Beneutron_det_100 = beryllium_neutrons_100mev[[\"HomogeneousBeT\",\"Layer 1BeT\",\"Layer 2BeT\", \"Layer 3BeT\",\"Layer 4BeT\",\"Layer 5BeT\"]].sum(axis=1)\n",
    "E_Bephoton_det_100 = beryllium_photons_100mev[[\"HomogeneousBeT\",\"Layer 1BeT\",\"Layer 2BeT\", \"Layer 3BeT\",\"Layer 4BeT\",\"Layer 5BeT\"]].sum(axis=1)\n",
    "E_Betotal_det_100 = pd.concat([E_Beelectron_det_100,E_Beproton_det_100,E_Beneutron_det_100,E_Bephoton_det_100],\\\n",
    "                           ignore_index=True)\n",
    "\n",
    "E_Beelectron_det_10g = beryllium_electrons_10gev[[\"Homogeneous\",\"Layer 1\",\"Layer 2\", \"Layer 3\",\"Layer 4\",\"Layer 5\"]].sum(axis=1)\n",
    "E_Beproton_det_10g = beryllium_protons_10gev[[\"Homogeneous\",\"Layer 1\",\"Layer 2\", \"Layer 3\",\"Layer 4\",\"Layer 5\"]].sum(axis=1)\n",
    "E_Beneutron_det_10g = beryllium_neutrons_10gev[[\"Homogeneous\",\"Layer 1\",\"Layer 2\", \"Layer 3\",\"Layer 4\",\"Layer 5\"]].sum(axis=1)\n",
    "E_Bephoton_det_10g = beryllium_photons_10gev[[\"Homogeneous\",\"Layer 1\",\"Layer 2\", \"Layer 3\",\"Layer 4\",\"Layer 5\"]].sum(axis=1)\n",
    "E_Betotal_det_10g = pd.concat([E_Beelectron_det_10g,E_Beproton_det_10g,E_Beneutron_det_10g,E_Bephoton_det_10g],\\\n",
    "                           ignore_index=True)\n",
    "\n",
    "# Remove zero values to avoid divide by 0 errors\n",
    "E_Beelectron_det_100 = E_Beelectron_det_100.drop(np.where(E_Beelectron_det_100==0)[0])\n",
    "E_Beproton_det_100 = E_Beproton_det_100.drop(np.where(E_Beproton_det_100==0)[0])\n",
    "E_Beneutron_det_100 = E_Beneutron_det_100.drop(np.where(E_Beneutron_det_100==0)[0])\n",
    "E_Bephoton_det_100 = E_Bephoton_det_100.drop(np.where(E_Bephoton_det_100==0)[0])\n",
    "E_Betotal_det_100 = E_Betotal_det_100.drop(np.where(E_Betotal_det_100==0)[0])\n",
    "\n",
    "E_Beelectron_det_10g = E_Beelectron_det_10g.drop(np.where(E_Beelectron_det_10g==0)[0])\n",
    "E_Beproton_det_10g = E_Beproton_det_10g.drop(np.where(E_Beproton_det_10g==0)[0])\n",
    "E_Beneutron_det_10g = E_Beneutron_det_10g.drop(np.where(E_Beneutron_det_10g==0)[0])\n",
    "E_Bephoton_det_10g = E_Bephoton_det_10g.drop(np.where(E_Bephoton_det_10g==0)[0])\n",
    "E_Betotal_det_10g = E_Betotal_det_10g.drop(np.where(E_Betotal_det_10g==0)[0])\n",
    "\n",
    "# Calibrate the energy\n",
    "E_Beelectron_cal_100 = np.mean(E_true_100[:len(E_Beelectron_det_100)]/np.asarray(E_Beelectron_det_100))*E_Beelectron_det_100 \n",
    "E_Beproton_cal_100 = np.mean(E_true_100[:len(E_Beproton_det_100)]/np.asarray(E_Beproton_det_100))*E_Beproton_det_100\n",
    "E_Beneutron_cal_100 = np.mean(E_true_100[:len(E_Beneutron_det_100)]/np.asarray(E_Beneutron_det_100))*E_Beneutron_det_100\n",
    "E_Bephoton_cal_100 = np.mean(E_true_100[:len(E_Bephoton_det_100)]/np.asarray(E_Bephoton_det_100))*E_Bephoton_det_100\n",
    "E_Betotal_cal_100 = np.mean(np.asarray(4*list(E_true_100))[:len(E_Betotal_det_100)]/np.asarray(E_Betotal_det_100))*E_Betotal_det_100\n",
    "\n",
    "E_Beelectron_cal_10g = np.mean(E_true_10g[:len(E_Beelectron_det_10g)]/np.asarray(E_Beelectron_det_10g))*E_Beelectron_det_10g \n",
    "E_Beproton_cal_10g = np.mean(E_true_10g[:len(E_Beproton_det_10g)]/np.asarray(E_Beproton_det_10g))*E_Beproton_det_10g\n",
    "E_Beneutron_cal_10g = np.mean(E_true_10g[:len(E_Beneutron_det_10g)]/np.asarray(E_Beneutron_det_10g))*E_Beneutron_det_10g\n",
    "E_Bephoton_cal_10g = np.mean(E_true_10g[:len(E_Bephoton_det_10g)]/np.asarray(E_Bephoton_det_10g))*E_Bephoton_det_10g\n",
    "E_Betotal_cal_10g = np.mean(np.asarray(4*list(E_true_10g))[:len(E_gtotal_det_10g)]/np.asarray(E_Betotal_det_10g))*E_Betotal_det_10g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bfcff5f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Calculating the weighted differences of the calibrated and true energy\n",
    "electron_Bediff_100 = (np.asarray(E_Beelectron_cal_100)-E_true_100[:len(E_Beelectron_det_100)])/E_true_100[:len(E_Beelectron_det_100)]\n",
    "proton_Bediff_100 = (np.asarray(E_Beproton_cal_100)-E_true_100[:len(E_Beproton_det_100)])/E_true_100[:len(E_Beproton_det_100)]\n",
    "neutron_Bediff_100 = (np.asarray(E_Beneutron_cal_100)-E_true_100[:len(E_Beneutron_det_100)])/E_true_100[:len(E_Beneutron_det_100)]\n",
    "photon_Bediff_100 = (np.asarray(E_Bephoton_cal_100)-E_true_100[:len(E_Bephoton_det_100)])/E_true_100[:len(E_Bephoton_det_100)]\n",
    "total_Bediff_100 = (np.asarray(E_Betotal_cal_100)-np.asarray(4*list(E_true_100))[:len(E_Betotal_det_100)])\\\n",
    "                    /np.asarray(4*list(E_true_100))[:len(E_Betotal_det_100)]\n",
    "\n",
    "electron_Bediff_10g = (np.asarray(E_Beelectron_cal_10g)-E_true_10g[:len(E_Beelectron_det_10g)])/E_true_10g[:len(E_Beelectron_det_10g)]\n",
    "proton_Bediff_10g = (np.asarray(E_Beproton_cal_10g)-E_true_10g[:len(E_Beproton_det_10g)])/E_true_10g[:len(E_Beproton_det_10g)]\n",
    "neutron_Bediff_10g = (np.asarray(E_Beneutron_cal_10g)-E_true_10g[:len(E_Beneutron_det_10g)])/E_true_10g[:len(E_Beneutron_det_10g)]\n",
    "photon_Bediff_10g = (np.asarray(E_Bephoton_cal_10g)-E_true_10g[:len(E_Bephoton_det_10g)])/E_true_10g[:len(E_Bephoton_det_10g)]\n",
    "total_Bediff_10g = (np.asarray(E_Betotal_cal_10g)-np.asarray(4*list(E_true_10g))[:len(E_Betotal_det_10g)])\\\n",
    "                    /np.asarray(4*list(E_true_10g))[:len(E_Betotal_det_10g)]\n",
    "\n",
    "# Calculating the energy resolution\n",
    "electron_Beres_100 = np.std(electron_Bediff_100)\n",
    "proton_Beres_100 = np.std(proton_Bediff_100)\n",
    "neutron_Beres_100 = np.std(neutron_Bediff_100)\n",
    "photon_Beres_100 = np.std(photon_Bediff_100)\n",
    "total_Beres_100 = np.std(total_Bediff_100)\n",
    "\n",
    "electron_Beres_10g = np.std(electron_Bediff_10g)\n",
    "proton_Beres_10g = np.std(proton_Bediff_10g)\n",
    "neutron_Beres_10g = np.std(neutron_Bediff_10g)\n",
    "photon_Beres_10g = np.std(photon_Bediff_10g)\n",
    "total_Beres_10g = np.std(total_Bediff_10g)\n",
    "\n",
    "#Plotting the Histograms\n",
    "fig,ax = plt.subplots(2,2,figsize=(12,9))\n",
    "\n",
    "ax[0,0].hist(electron_Bediff_100,label='100 MeV Resolution = {:.4f}'.format(electron_Beres_100))\n",
    "ax[0,0].hist(electron_Bediff_10g,label='10 GeV Resolution = {:.4f}'.format(electron_Beres_10g),alpha=0.8)\n",
    "ax[0,0].set_title(\"Calibration Energy histogram of Electrons\")\n",
    "ax[0,0].set_xlabel(\"($E_{cal}-E_{true})/E_{true}$\")\n",
    "ax[0,0].set_ylabel(\"Frequency\")\n",
    "ax[0,0].legend()\n",
    "\n",
    "ax[1,0].hist(proton_Bediff_100,label='100 MeV Resolution = {:.4f}'.format(proton_Beres_100))\n",
    "ax[1,0].hist(proton_Bediff_10g,label='10 GeV Resolution = {:.4f}'.format(proton_Beres_10g))\n",
    "ax[1,0].set_title(\"Calibration Energy histogram of Protons\")\n",
    "ax[1,0].set_xlabel(\"($E_{cal}-E_{true})/E_{true}$\")\n",
    "ax[1,0].set_ylabel(\"Frequency\")\n",
    "ax[1,0].legend()\n",
    "\n",
    "ax[0,1].hist(neutron_Bediff_100,label='100 MeV Resolution = {:.4f}'.format(neutron_Beres_100))\n",
    "ax[0,1].hist(neutron_Bediff_10g,label='10 GeV Resolution = {:.4f}'.format(neutron_Beres_10g),alpha=0.8)\n",
    "ax[0,1].set_title(\"Calibration Energy histogram of Neutrons\")\n",
    "ax[0,1].set_xlabel(\"($E_{cal}-E_{true})/E_{true}$\")\n",
    "ax[0,1].set_ylabel(\"Frequency\")\n",
    "ax[0,1].set_xscale('log')\n",
    "ax[0,1].legend()\n",
    "\n",
    "ax[1,1].hist(photon_Bediff_100,label='100 MeV Resolution = {:.4f}'.format(photon_Beres_100))\n",
    "ax[1,1].hist(photon_Bediff_10g,label='10 GeV Resolution = {:.4f}'.format(photon_Beres_10g))\n",
    "ax[1,1].set_title(\"Calibration Energy histogram of Photons\")\n",
    "ax[1,1].set_xlabel(\"($E_{cal}-E_{true})/E_{true}$\")\n",
    "ax[1,1].set_ylabel(\"Frequency\")\n",
    "ax[1,1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.hist(total_Bediff_100,label='100 MeV Resolution = {:.4f}'.format(total_Beres_100))\n",
    "plt.hist(total_Bediff_10g,label='10 GeV Resolution = {:.4f}'.format(total_Beres_10g),alpha=0.8)\n",
    "plt.title(\"Calibration Energy histogram of all Particles\")\n",
    "plt.xlabel(\"($E_{cal}-E_{true})/E_{true}$\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.xscale('log')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2652f901",
   "metadata": {},
   "source": [
    "For 100 MeV, the overall energy resolution is much better than tungsten although much worse than graphite by a factor 2. Again, the high neutron resolution dominates the total resolution but the individual resolutions show that photons, electrons and protons do have a higher resolution than graphite. This more or less disproves the theory mentioned above (where photon pair production causes the confusion) as if it were the case, then a worse electron and photon resolution would be expected here. This is not the case. Thus, it appears that graphite is not (marginally) better than beryllium due to the amount of particles that elastically scatter but by the number of particles that inelastically scatter. Thus, using a fixed target experiment on a linear detector, it appears that the quality of particle classification is inversely proportional to the energy resolution.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "455462ba",
   "metadata": {},
   "source": [
    "## Putting all data together\n",
    " In this section, the possibility of improving particle classification by using all of the target data simultaneously is investigated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8450a95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenating all the target data\n",
    "all_electrons_100mev = pd.concat([electrons_100mev,electrons_100mev_Ctarget,electrons_100mev_Wtarget,\\\n",
    "                                 electrons_100mev_Betarget],axis=1)\n",
    "all_protons_100mev = pd.concat([protons_100mev,protons_100mev_Ctarget,protons_100mev_Wtarget,\\\n",
    "                               protons_100mev_Betarget],axis=1)\n",
    "all_neutrons_100mev = pd.concat([neutrons_100mev,neutrons_100mev_Ctarget,neutrons_100mev_Wtarget,\\\n",
    "                                neutrons_100mev_Betarget],axis=1)\n",
    "all_photons_100mev = pd.concat([photons_100mev,photons_100mev_Ctarget,photons_100mev_Wtarget,\\\n",
    "                               photons_100mev_Betarget],axis=1)\n",
    "all_electrons_10gev = pd.concat([electrons_10gev,electrons_10gev_Ctarget,electrons_10gev_Wtarget,\\\n",
    "                                electrons_10gev_Betarget],axis=1)\n",
    "all_protons_10gev = pd.concat([protons_10gev,protons_10gev_Ctarget,protons_10gev_Wtarget,\\\n",
    "                              protons_10gev_Betarget],axis=1)\n",
    "all_neutrons_10gev = pd.concat([neutrons_10gev,neutrons_10gev_Ctarget,neutrons_10gev_Wtarget,\\\n",
    "                               neutrons_10gev_Betarget],axis=1)\n",
    "all_photons_10gev = pd.concat([photons_10gev,photons_10gev_Ctarget,photons_10gev_Wtarget,\\\n",
    "                              photons_10gev_Betarget],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6a3e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Appending them all together\n",
    "all_particles_100mev = pd.concat([all_electrons_100mev,all_protons_100mev,all_neutrons_100mev,\\\n",
    "                                  all_photons_100mev],ignore_index=True)\n",
    "all_particles_10gev = pd.concat([all_electrons_10gev,all_protons_10gev,all_neutrons_10gev,\\\n",
    "                                 all_photons_10gev],ignore_index=True)\n",
    "\n",
    "#Randomly shuffling\n",
    "all_particles_100mev = all_particles_100mev.sample(frac=1).reset_index(drop=True)\n",
    "all_particles_10gev = all_particles_10gev.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba45faa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check to see if our data frame looks good\n",
    "all_particles_100mev\n",
    "# It looks great!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aecf2fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating the testing and training data sets\n",
    "\n",
    "all_y_100 = all_particles_100mev[[\"Particle ID\"]]\n",
    "all_x_100 = all_particles_100mev[[\"Homogeneous\",\"Layer 1\",\"Layer 2\", \"Layer 3\",\"Layer 4\",\"Layer 5\",\\\n",
    "                           \"HomogeneousCT\", \"Layer 1CT\",\"Layer 2CT\", \"Layer 3CT\",\"Layer 4CT\",\"Layer 5CT\",\\\n",
    "                           \"HomogeneousWT\", \"Layer 1WT\",\"Layer 2WT\", \"Layer 3WT\",\"Layer 4WT\",\"Layer 5WT\",\\\n",
    "                           \"HomogeneousBeT\", \"Layer 1BeT\",\"Layer 2BeT\", \"Layer 3BeT\",\"Layer 4BeT\",\"Layer 5BeT\"]]\n",
    "\n",
    "all_y_10 = all_particles_10gev[[\"Particle ID\"]]\n",
    "all_x_10 = all_particles_10gev[[\"Homogeneous\",\"Layer 1\",\"Layer 2\", \"Layer 3\",\"Layer 4\",\"Layer 5\",\\\n",
    "                           \"HomogeneousCT\", \"Layer 1CT\",\"Layer 2CT\", \"Layer 3CT\",\"Layer 4CT\",\"Layer 5CT\",\\\n",
    "                           \"HomogeneousWT\", \"Layer 1WT\",\"Layer 2WT\", \"Layer 3WT\",\"Layer 4WT\",\"Layer 5WT\",\\\n",
    "                           \"HomogeneousBeT\", \"Layer 1BeT\",\"Layer 2BeT\", \"Layer 3BeT\",\"Layer 4BeT\",\"Layer 5BeT\"]]\n",
    "\n",
    "all_x100m_train, all_x100m_test, all_y100m_train, all_y100m_test = train_test_split(all_x_100,all_y_100, test_size=0.3,\n",
    "                                                        random_state=1) # 70% training and 30% test\n",
    "\n",
    "all_x10g_train, all_x10g_test, all_y10g_train, all_y10g_test = train_test_split(all_x_10,all_y_10, test_size=0.3,\n",
    "                                                        random_state=1) # 70% training and 30% test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ee499f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the same model architecture as for all the others\n",
    "model_all_100m = model(np.shape(all_x100m_train)[1])\n",
    "model_all_10g = model(np.shape(all_x10g_train)[1])\n",
    "\n",
    "model_all_100m.summary()\n",
    "# This has the expected increase in parameters so all is well "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2dfb725",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use 100 epochs with 20% validation data and a batch size of 128\n",
    "history_all_100m = model_all_100m.fit(x=all_x100m_train,y=all_y100m_train,batch_size=128,\\\n",
    "                                              epochs=100,validation_split=0.2,shuffle=True)\n",
    "\n",
    "# Use 100 epochs with 20% validation data and a batch size of 128\n",
    "history_all_10g = model_all_10g.fit(x=all_x10g_train,y=all_y10g_train,batch_size=128,\\\n",
    "                                              epochs=100,validation_split=0.2,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9eabf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting the loss curves\n",
    "\n",
    "fig,ax = plt.subplots(2,2,figsize=(12,9))\n",
    "ax[0,0].plot(history_all_100m.history['loss'],label='Loss')\n",
    "ax[0,0].plot(history_all_100m.history['val_loss'],label='Validation Loss')\n",
    "ax[0,0].set_title(\"Loss Change over Time (100 MeV)\")\n",
    "ax[0,0].set_xlabel(\"Epoch\")\n",
    "ax[0,0].set_ylabel(\"Loss\")\n",
    "ax[0,0].legend()\n",
    "\n",
    "ax[1,0].plot(history_all_100m.history['accuracy'],label='Accuracy')\n",
    "ax[1,0].plot(history_all_100m.history['val_accuracy'],label='Validation Accuracy')\n",
    "ax[1,0].set_title(\"Accuracy Change over Time (100 MeV)\")\n",
    "ax[1,0].set_xlabel(\"Epoch\")\n",
    "ax[1,0].set_ylabel(\"Accuracy\")\n",
    "ax[1,0].legend()\n",
    "\n",
    "ax[0,1].plot(history_all_10g.history['loss'],label='Loss')\n",
    "ax[0,1].plot(history_all_10g.history['val_loss'],label='Validation Loss')\n",
    "ax[0,1].set_title(\"Loss Change over Time (10 GeV)\")\n",
    "ax[0,1].set_xlabel(\"Epoch\")\n",
    "ax[0,1].set_ylabel(\"Loss\")\n",
    "ax[0,1].legend()\n",
    "\n",
    "ax[1,1].plot(history_all_10g.history['accuracy'],label='Accuracy')\n",
    "ax[1,1].plot(history_all_10g.history['val_accuracy'],label='Validation Accuracy')\n",
    "ax[1,1].set_title(\"Accuracy Change over Time (10 GeV)\")\n",
    "ax[1,1].set_xlabel(\"Epoch\")\n",
    "ax[1,1].set_ylabel(\"Accuracy\")\n",
    "ax[1,1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0fe17b",
   "metadata": {},
   "source": [
    "These loss curves are vastly superior to all the others; they show no signs of overtraining, and both have an accuracy of around 90%. Like the others (except for tungsten), the accuracy is higher for low energies than for high energies which is probably due to the fact that the combination of beryllium and graphite were instrumental in differentiating them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1604e6a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting the testing data sets\n",
    "prediction_all_100m = np.argmax(model_all_100m.predict(all_x100m_test),axis=1)\n",
    "prediction_all_10g = np.argmax(model_all_10g.predict(all_x10g_test),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a631287",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "fig,ax = plt.subplots(1,2,figsize=(12,5))\n",
    "#Define the confusion matrix and normalise it so that each row/column sums to 1\n",
    "all_cmat100m = confusion_matrix(all_y100m_test,prediction_all_100m,normalize='true')\n",
    "all_cmat10g = confusion_matrix(all_y10g_test,prediction_all_10g,normalize='true')\n",
    "#dispay the matrix and redefine the display labels to show the particle type\n",
    "all_cmatplot100m = ConfusionMatrixDisplay(confusion_matrix=all_cmat100m,display_labels=['electron','proton','neutron','photon'])\n",
    "all_cmatplot10g = ConfusionMatrixDisplay(confusion_matrix=all_cmat10g,display_labels=['electron','proton','neutron','photon'])\n",
    "\n",
    "all_cmatplot100m.plot(ax=ax[0])\n",
    "ax[0].set_title(\"100 MeV\")\n",
    "all_cmatplot10g.plot(ax=ax[1])\n",
    "ax[1].set_title(\"10 GeV\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1fa352",
   "metadata": {},
   "source": [
    "These confusion matrices are much better than all the others for both energies. At 100 MeV, the distinction of photons is at 100% and the neutrons are almost as high. With an 80% accuracy of both electrons and photons, it can be seen that the combination of the 4 data types were better than when each were separate. This is to be expected as there is more data but the large increase is also to do with the interactions that occured on the targets. Looking at the energy calibration, it appears more energy was deposited in the target by neutrons when it was beryllium (resolution of 14 vs 18 for beryllium) and more energy was deposited by photons, electrons and protons when it was graphite. Thus, by combining the data, all particles could be looked at and the high energy confusion matrix was somewhat helped by the tungsten which only let through the highest energy particles.\n",
    "\n",
    "The photons and electrons are still getting confused but this is a classic issue in particle physics as they do interact very similarly. As a further extension, adding a magnetic field and a tracker would be even better as then curvature could be used on top of energy calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3acf3a4e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
